{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now focus on optimizing a LlamaIndex RAG pipeline through a series of iterative evaluations. The ste-by-step plan is: \n",
    "\n",
    "-   **Baseline Evaluation**: Construct a standard LlamaIndex RAG pipeline and establish an initial performance baseline. We will adjust the top-k retrieval values to understand their effects on the accuracy and relevance of generated answers.\n",
    "-   **Testing Different Embedding Models**: Evaluate different embedding models to identify the most effective on efor our pipeline.\n",
    "-   **Incorporating a Reranker**: Implement a reranking mechanism to refine the document selection process of the retriever.\n",
    "-   **Employing a Deep memory Approach**: Investigate the impact of a deep memory component on the accuracy of information retrieval.*(This part requires a paid subscription to ActiveLoop)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import os\n",
    "\n",
    "file_path = os.path.join('paul_graham', 'paul_graham_essay.txt')\n",
    "wget.download('https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt', out=file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the setting to execute the open-source models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "Settings.embed_model = OllamaEmbeddings(model=\"llama3.1:8b\") # Load it into the setting of llama index\n",
    "Settings.llm = OllamaLLM(model=\"llama3.1:8b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Llamaindex nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# Load the download documents\n",
    "documents = SimpleDirectoryReader(\"./paul_graham/\").load_data()\n",
    "node_parser = SimpleNodeParser.from_defaults(chunk_size=512)\n",
    "# Get nodes from documents\n",
    "nodes = node_parser.get_nodes_from_documents(documents=documents)\n",
    "\n",
    "# By default, the node/chunks ids are set to random uuids. To ensure same id's per run, we manually set them.\n",
    "for idx, node in enumerate(nodes):\n",
    "    node.id_ = f\"node_{idx}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a LlamaIndex `VectorStoreIndex` and store the embeddings into `DeepLakeVectorStore`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 129/129 [00:43<00:00,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading data to deeplake dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 129/129 [00:00<00:00, 295.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./ddbb/paul_graham', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype       shape      dtype  compression\n",
      "  -------    -------     -------    -------  ------- \n",
      "   text       text      (129, 1)      str     None   \n",
      " metadata     json      (129, 1)      str     None   \n",
      " embedding  embedding  (129, 4096)  float32   None   \n",
      "    id        text      (129, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.vector_stores.deeplake import DeepLakeVectorStore\n",
    "from llama_index.core import ServiceContext, StorageContext, VectorStoreIndex\n",
    "\n",
    "# Create a local deep Deep Lake VectorStore\n",
    "dataset_path = \"./ddbb/paul_graham\"\n",
    "vector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=True)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# Create and save the embeddings from the nodes\n",
    "vector_index = VectorStoreIndex(nodes, settings=Settings, storage_context=storage_context, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build a `QueryEngine`, which generates answers with the LLM and the retrieved chunks of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main things Paul worked on before college were:\n",
      "\n",
      "* Learning programming through interactions with friends who had microcomputers (specifically a Heathkit kit and later a TRS-80)\n",
      "* Writing simple programs for his own use, such as games, model rocket predictions, and a word processor\n",
      "* Assembling and using a home-built computer kit\n",
      "\n",
      "Note: The original context of Paul's essay was about how he learned to program before college, but it also mentioned that philosophy was his intended college major. However, the new context provided (which is actually the same as the original text) further emphasizes Paul's early interest in programming and his continued engagement with it throughout his life.\n"
     ]
    }
   ],
   "source": [
    "query_engine = vector_index.as_query_engine(similarity_top_k=10)\n",
    "response_vector = query_engine.query(\"What are the main things Paul worked on before college?\")\n",
    "print(response_vector.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a simple RAG pipeline, we can evaluate it. For that, we need a dataset. `LlamaIndex` offers a `generate_question_context_pairs` module specifically for generating questions and context pairs. We will use that dataset to assess the RAG chunk retrieval and response capabilities.\n",
    "\n",
    "Let’s also save the generated dataset in JSON format for later use. In this case we only generate 58 question and context pairs, but you can increase the number of samples in the dataset for a more thorough evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 129/129 [12:11<00:00,  5.67s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import generate_question_context_pairs\n",
    "\n",
    "qc_dataset = generate_question_context_pairs(\n",
    "    nodes,\n",
    "    llm=Settings.llm,\n",
    "    num_questions_per_chunk=1\n",
    ")\n",
    "# Save the results\n",
    "qc_dataset.save_json(\"qc_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset. We have generated 128 questions, one for each chunk as we set in `num_questions_per_chunk`. This questions are about the documents we have loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
    "\n",
    "# Load the results\n",
    "qc_dataset = EmbeddingQAFinetuneDataset.from_json(\n",
    "    \"qc_dataset.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we start with the retrieval evaluations. We will use the `RetrieverEvaluator` class available in LlamaIndex to measure:\n",
    "\n",
    "-   *Hit Rate*: Measures how often you guess the correct answer by only looking at your top few guesses. You have a high hit rate if you often find the right answer in your first few guesses.So, in a retrieval system, it's about how frequently the system finds the correct document within its top 'k' picks.\n",
    "-   *Mean Reciprocal Rank(MMR)*: For a retrieval system, MRR looks at where the correct document ranks in the system's guesses. If it's usually near the top, the MRR will be high, indicating good performance. \n",
    "\n",
    "In summary, **Hit Rate tells you how often the system gets it right in its top guesses, and MRR tells you how close to the top the right answer usually is**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Retriever Name  Hit Rate       MRR\n",
      "0  Retriever top_2  0.015504  0.007752\n",
      "    Retriever Name  Hit Rate      MRR\n",
      "0  Retriever top_4  0.031008  0.01292\n",
      "    Retriever Name  Hit Rate      MRR\n",
      "0  Retriever top_6   0.03876  0.01447\n",
      "    Retriever Name  Hit Rate       MRR\n",
      "0  Retriever top_8  0.046512  0.015578\n",
      "     Retriever Name  Hit Rate     MRR\n",
      "0  Retriever top_10  0.062016  0.0173\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "\n",
    "def display_results_retriever(name, eval_results):\n",
    "    \"\"\"Display results from the evaluate\"\"\"\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "    hit_rate = full_df[\"hit_rate\"].mean()\n",
    "    mrr = full_df[\"mrr\"].mean()\n",
    "    metric_df = pd.DataFrame(\n",
    "        {\"Retriever Name\": [name], \"Hit Rate\": [hit_rate], \"MRR\": [mrr]}\n",
    "    )\n",
    "\n",
    "    return metric_df\n",
    "\n",
    "for i in [2,4,6,8,10]:\n",
    "    # Create a retriever that returns the top-k similar nodes\n",
    "    retriever = vector_index.as_retriever(similarity_top_k=i)\n",
    "    # Create a retriever evaluator object\n",
    "    retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "        [\"mrr\", \"hit_rate\"], retriever=retriever\n",
    "    )\n",
    "    eval_results = await retriever_evaluator.aevaluate_dataset(qc_dataset)\n",
    "    print(display_results_retriever(f\"Retriever top_{i}\", eval_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the hit rate increases as the top_k value increases. But how does that impact the quality of the generated asnwers?\n",
    "\n",
    "Now, we will evaluate **relevancy** and **faithfulness**:\n",
    "\n",
    "-   *Relevancy*: Evaluates whether the retrieved context and answer is relevant to the query.\n",
    "-   *Faithfulness*: Evaluates if the answer is faithful of if there's hallucination.\n",
    "\n",
    "To execute this, we will use a bigger open-source model, **Mistral-Nemo 12b**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'model_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m llm_mistral \u001b[38;5;241m=\u001b[39m OllamaLLM(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral-nemo:latest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Evaluating systems with a bigger LLM model\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m faithfulness_evaluator \u001b[38;5;241m=\u001b[39m \u001b[43mFaithfulnessEvaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_mistral\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m relevancy_evaluator \u001b[38;5;241m=\u001b[39m RelevancyEvaluator(llm\u001b[38;5;241m=\u001b[39mllm_mistral)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Queries\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aleja\\OneDrive\\Escritorio\\Cursos\\RAG Course ActiveLoop\\RAGCourse\\Lib\\site-packages\\llama_index\\core\\evaluation\\faithfulness.py:133\u001b[0m, in \u001b[0;36mFaithfulnessEvaluator.__init__\u001b[1;34m(self, llm, raise_error, eval_template, refine_template)\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_template \u001b[38;5;241m=\u001b[39m eval_template\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 133\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_template \u001b[38;5;241m=\u001b[39m TEMPLATES_CATALOG\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    135\u001b[0m         model_name, DEFAULT_EVAL_TEMPLATE\n\u001b[0;32m    136\u001b[0m     )\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_refine_template: BasePromptTemplate\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'model_name'"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import RelevancyEvaluator, FaithfulnessEvaluator, BatchEvalRunner\n",
    "\n",
    "# Ajustar el ciclo para evitar el uso de ServiceContext\n",
    "for i in [2, 4, 6, 8, 10]:\n",
    "    # Query engine with the Ollama model\n",
    "    query_engine = vector_index.as_query_engine(similarity_top_k=i)\n",
    "\n",
    "    \n",
    "    llm_mistral = OllamaLLM(model=\"mistral-nemo:latest\")\n",
    "\n",
    "    # Evaluating systems with a bigger LLM model\n",
    "    faithfulness_evaluator = FaithfulnessEvaluator(llm=llm_mistral)\n",
    "    relevancy_evaluator = RelevancyEvaluator(llm=llm_mistral)\n",
    "\n",
    "    # Queries\n",
    "    queries = list(qc_dataset.queries.values())\n",
    "    batch_eval_queries = queries[:20]\n",
    "\n",
    "    # Configurate evaluator by batches with the Mistral 12b model\n",
    "    runner = BatchEvalRunner(\n",
    "        {\"faithfulness\": faithfulness_evaluator, \"relevancy\": relevancy_evaluator},\n",
    "        workers=8,\n",
    "    )\n",
    "\n",
    "    # Execute evaluations\n",
    "    eval_results = await runner.aevaluate_queries(\n",
    "        query_engine, # Query engine with the Ollama 8B model\n",
    "        queries=batch_eval_queries\n",
    "    )\n",
    "\n",
    "    # Calculate and show the results\n",
    "    faithfulness_score = sum(result.passing for result in eval_results['faithfulness']) / len(eval_results['faithfulness'])\n",
    "    print(f\"top_{i} faithfulness_score: {faithfulness_score}\")\n",
    "\n",
    "    relevancy_score = sum(result.passing for result in eval_results['relevancy']) / len(eval_results['relevancy'])\n",
    "    print(f\"top_{i} relevancy_score: {relevancy_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGCourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
