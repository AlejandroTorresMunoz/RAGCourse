{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will see how to use all the concepts learnt in the past 2 notebooks to connect with Github repositories.\n",
    "\n",
    "The following elements are going to be used : \n",
    "-   **LlamaIndex** : Data framework.\n",
    "    -   *Data Connectors* : Ingest data from various sources.\n",
    "    -   *Data Indexing* : Structure the ingested data.\n",
    "    -   *Query Engines* : Enable natural language queries to interact with the stored data.\n",
    "-   **DeepLake** : Data lake.\n",
    "    -   *Optimized Storage* : Designed for quick data retrieval.\n",
    "    -   *Data Type Support* : Handles multiple data tyoes, like images, videos and complex data structures.\n",
    "-   **Ollama** : LLM models.\n",
    "-   **python-dotenv** : Library to specify environment variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to set the values of the enviroment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set LlamaIndex settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "Settings.embed_model = OllamaEmbeddings(model=\"llama3.1:8b\") # Load it into the setting of llama index\n",
    "Settings.llm = OllamaLLM(model=\"llama3.1:8b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to change the following code to load your keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "with open('../data/keys.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    github_token = data['GitHubToken']\n",
    "    activeloop_token = data['ActiveLoopKey']\n",
    "    github_url = data['GitHubUrl']\n",
    "    activeloop_url = data['ActiveLoopVectorStoreUrl']\n",
    "    os.environ[\"GITHUB_TOKEN\"] = github_token\n",
    "    os.environ[\"ACTIVELOOP_TOKEN\"] = activeloop_token\n",
    "    os.environ[\"GITHUB_PATH\"] = github_url\n",
    "    os.environ[\"ACTIVELOOP_PATH\"] = activeloop_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleja\\AppData\\Local\\Temp\\ipykernel_24964\\796836447.py:30: DeprecationWarning: Call to deprecated function (or staticmethod) download_loader. (`download_loader()` is deprecated. Please install tool using pip install directly instead.)\n",
      "  download_loader(\"GithubRepositoryReader\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RAGCourse repository by AlejandroTorresMunoz\n",
      "Documents uploaded : \n",
      "{'file_path': 'README.md', 'file_name': 'README.md', 'url': 'https://github.com/AlejandroTorresMunoz\\\\RAGCourse\\\\blob/main\\\\README.md'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from llama_index.readers.github import GithubRepositoryReader, GithubClient\n",
    "from llama_index.core import download_loader\n",
    "\n",
    "def parse_github_url(url):\n",
    "    # Function that takes a GitHub URL and extracts the repository owner and name using regular expressions\n",
    "    pattern = r\"https://github\\.com/([^/]+)/([^/]+)\"\n",
    "    match = re.match(pattern, url)\n",
    "    return match.groups() if match else (None, None)\n",
    "\n",
    "def validate_owner_repo(owner, repo):\n",
    "    # Function that both the repository owner and name are present\n",
    "    return bool(owner) and bool(repo)\n",
    "\n",
    "def initialize_github_client():\n",
    "    # Initializes the GitHub client using the token fetched from the environment variables\n",
    "    github_token = os.getenv(\"GITHUB_TOKEN\")\n",
    "    return GithubClient(github_token)\n",
    "\n",
    "# Check for GitHub Token\n",
    "github_token = os.getenv(\"GITHUB_TOKEN\")\n",
    "if not github_token:\n",
    "    raise EnvironmentError(\"GitHub token not found in environment variables\")\n",
    "# Check for Activeloop Token\n",
    "active_loop_token = os.getenv(\"ACTIVELOOP_TOKEN\")\n",
    "if not active_loop_token:\n",
    "    raise EnvironmentError(\"Activeloop token not found in environment variables\")\n",
    "\n",
    "github_client = initialize_github_client()\n",
    "download_loader(\"GithubRepositoryReader\")\n",
    "github_url = os.getenv(\"GITHUB_PATH\")\n",
    "owner, repo = parse_github_url(github_url) # Get the owner and repository name\n",
    "\n",
    "loader = GithubRepositoryReader(\n",
    "    github_client=github_client,\n",
    "    owner=owner,\n",
    "    repo=repo,\n",
    "    filter_file_extensions=([\".py\", \".md\"], GithubRepositoryReader.FilterType.INCLUDE),\n",
    "    verbose=False,\n",
    "    concurrent_requests=5\n",
    ")\n",
    "print(f\"Loading {repo} repository by {owner}\")\n",
    "docs = loader.load_data(branch=\"main\")\n",
    "print(\"Documents uploaded : \")\n",
    "for doc in docs:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data from GitHub has been downloaded, we create a vector store and upload the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating storage context\n",
      "Creating vector store index\n",
      "Uploading data to deeplake dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://alejandrotormun/repository_vector_store', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      "   text       text      (1, 1)      str     None   \n",
      " metadata     json      (1, 1)      str     None   \n",
      " embedding  embedding  (1, 4096)  float32   None   \n",
      "    id        text      (1, 1)      str     None   \n",
      "Creating query engine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    }
   ],
   "source": [
    "from llama_index.vector_stores.deeplake import DeepLakeVectorStore\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Create an object to connect with the vector store in ActiveLoop\n",
    "vector_store = DeepLakeVectorStore(\n",
    "    dataset_path=os.environ[\"ACTIVELOOP_PATH\"],\n",
    "    overwrite=True,\n",
    "    runtime={\"tensor_db\": True},\n",
    ")\n",
    "print(\"Creating storage context\")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store) # Storage context to store nodes, indexes, vectors...\n",
    "print(\"Creating vector store index\")\n",
    "index = VectorStoreIndex.from_documents(docs, storage_context=storage_context) # Vector store index\n",
    "print(\"Creating query engine\")\n",
    "query_engine = index.as_query_engine() # Create query engine based on the index created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the created query engine with a simple question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question : What is the repository about?\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aleja\\OneDrive\\Escritorio\\Cursos\\RAG Course ActiveLoop\\RAGCourse\\Lib\\site-packages\\llama_index\\llms\\langchain\\base.py:106: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  output_str = self._llm.predict(prompt, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: This repository is about taking annotations of a course on Foundational Model Certification with\n",
      "Ollama models (open-source models), which is different from another existing course. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "intro_question = \"What is the repository about?\"\n",
    "print(f\"Test question : {intro_question}\")\n",
    "print(\"=\" * 50)\n",
    "answer = query_engine.query(intro_question)\n",
    "print(f\"Answer: {textwrap.fill(str(answer), 100)} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diving Deeper : Low-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will learn how to customize the query logic we have seen in this first chapter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an index of the documents\n",
    "try:\n",
    "    # Create the store in ActiveLoop cloud\n",
    "    vector_store = DeepLakeVectorStore(\n",
    "        dataset_path=os.environ[\"ACTIVELOOP_PATH\"],\n",
    "        overwrite=True,\n",
    "        runtime={\"tensor_db\": True},\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while creating or fetching the vector store: {str(e)}\")\n",
    "\n",
    "storage_context = StorageContext.from_documents(vector_store=vector_store)\n",
    "# Load the documents into the vector store\n",
    "index = VectorStoreIndex.from_documents(docs, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retriever is the responsible for fetching relevant nodes from the index. LlamaIndex supports various retrieval modes, allowign you to choose the one that best fits your needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "# We configure a retriever that returns the 4 nodes with most similarity to the context \n",
    "retriever = VectorIndexRetriever(index=index, similarity_top_k=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also parse some extra parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "# Get an instance of the response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=retriever, # Retriver configured to select the most similar nodes\n",
    "    response_mode=\"default\", # Default mode means the system will \"create and refine\" an answer by sequentially going through each retrieved node\n",
    "    response_synthesizer=response_synthesizer, # Component that will generate the final response using the response_mode specified\n",
    "    node_postprocessors=[ # List of postprocessors that can be applied to the nodes given by the retriever\n",
    "        SimilarityPostprocessor(similarity_cutoff=0.7)]  # Filters out nodes based on their similarity scores, only nodes with a similarity score above 0.7 are considered \n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What code is in this repository?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can change the response mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "# Get an instance of the response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=retriever, # Retriver configured to select the most similar nodes\n",
    "    response_mode=\"compact\", # Compact mode means the system fits as many node text chunks as maximum prompt size. If there are too many chunks, it refines the answer by processing multiple prompts. \n",
    "    response_synthesizer=response_synthesizer, # Component that will generate the final response using the response_mode specified\n",
    "    node_postprocessors=[ # List of postprocessors that can be applied to the nodes given by the retriever\n",
    "        SimilarityPostprocessor(similarity_cutoff=0.7)]  # Filters out nodes based on their similarity scores, only nodes with a similarity score above 0.7 are considered \n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What code is in this repository?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "# Get an instance of the response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=retriever, # Retriver configured to select the most similar nodes\n",
    "    response_mode=\"tree_summarize\", # Tree summarize mode means the system constructs a tree from a set of nodes, and the query returns the root node as the response. It's beneficial for summarization nodes.  \n",
    "    response_synthesizer=response_synthesizer, # Component that will generate the final response using the response_mode specified\n",
    "    node_postprocessors=[ # List of postprocessors that can be applied to the nodes given by the retriever\n",
    "        SimilarityPostprocessor(similarity_cutoff=0.7)]  # Filters out nodes based on their similarity scores, only nodes with a similarity score above 0.7 are considered \n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What code is in this repository?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "# Get an instance of the response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=retriever, # Retriver configured to select the most similar nodes\n",
    "    response_mode=\"no_text\", # No text sumarrize mode means the retriever fetches the nodes but doesn't send them to the LLM. This mode allos to inspect the retrieved nodes  \n",
    "    response_synthesizer=response_synthesizer, # Component that will generate the final response using the response_mode specified\n",
    "    node_postprocessors=[ # List of postprocessors that can be applied to the nodes given by the retriever\n",
    "        SimilarityPostprocessor(similarity_cutoff=0.7)]  # Filters out nodes based on their similarity scores, only nodes with a similarity score above 0.7 are considered \n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What code is in this repository?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGCourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
