{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LlamaIndex is a framework designed to enhance the capabilities of LLM's by integrating RAG systems. Allos LLM-based application to get information using vector stores, nodes ...\n",
    "\n",
    "This folder will cover vector stores and their importance in semantic search, the role of data connectors and LlamaHub in data ingestion, the creation of node objects from documents, and the indexing of data for quick retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will use **document loaders**, a too ltha simplify the process of loading data into documents, and split texts into smaller chunks for better processing. Finally, the indexing process creates a structured database of information that the language model can query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content data : \n",
      "Index: 1\n",
      "Customer Id: DD37Cf93aecA6Dc\n",
      "First Name: Sheryl\n",
      "Last Name: Baxter\n",
      "Company: Rasmussen Group\n",
      "City: East Leonard\n",
      "Country: Chile\n",
      "Phone 1: 229.077.5154\n",
      "Phone 2: 397.884.0519x718\n",
      "Email: zunigavanessa@smith.info\n",
      "Subscription Date: 2020-08-24\n",
      "Website: http://www.stephenson.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 0}\n",
      "Content data : \n",
      "Index: 2\n",
      "Customer Id: 1Ef7b82A4CAAD10\n",
      "First Name: Preston\n",
      "Last Name: Lozano\n",
      "Company: Vega-Gentry\n",
      "City: East Jimmychester\n",
      "Country: Djibouti\n",
      "Phone 1: 5153435776\n",
      "Phone 2: 686-620-1820x944\n",
      "Email: vmata@colon.com\n",
      "Subscription Date: 2021-04-23\n",
      "Website: http://www.hobbs.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 1}\n",
      "Content data : \n",
      "Index: 3\n",
      "Customer Id: 6F94879bDAfE5a6\n",
      "First Name: Roy\n",
      "Last Name: Berry\n",
      "Company: Murillo-Perry\n",
      "City: Isabelborough\n",
      "Country: Antigua and Barbuda\n",
      "Phone 1: +1-539-402-0259\n",
      "Phone 2: (496)978-3969x58947\n",
      "Email: beckycarr@hogan.com\n",
      "Subscription Date: 2020-03-25\n",
      "Website: http://www.lawrence.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 2}\n",
      "Content data : \n",
      "Index: 4\n",
      "Customer Id: 5Cef8BFA16c5e3c\n",
      "First Name: Linda\n",
      "Last Name: Olsen\n",
      "Company: Dominguez, Mcmillan and Donovan\n",
      "City: Bensonview\n",
      "Country: Dominican Republic\n",
      "Phone 1: 001-808-617-6467x12895\n",
      "Phone 2: +1-813-324-8756\n",
      "Email: stanleyblackwell@benson.org\n",
      "Subscription Date: 2020-06-02\n",
      "Website: http://www.good-lyons.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 3}\n",
      "Content data : \n",
      "Index: 5\n",
      "Customer Id: 053d585Ab6b3159\n",
      "First Name: Joanna\n",
      "Last Name: Bender\n",
      "Company: Martin, Lang and Andrade\n",
      "City: West Priscilla\n",
      "Country: Slovakia (Slovak Republic)\n",
      "Phone 1: 001-234-203-0635x76146\n",
      "Phone 2: 001-199-446-3860x3486\n",
      "Email: colinalvarado@miles.net\n",
      "Subscription Date: 2021-04-17\n",
      "Website: https://goodwin-ingram.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 4}\n",
      "Content data : \n",
      "Index: 6\n",
      "Customer Id: 2d08FB17EE273F4\n",
      "First Name: Aimee\n",
      "Last Name: Downs\n",
      "Company: Steele Group\n",
      "City: Chavezborough\n",
      "Country: Bosnia and Herzegovina\n",
      "Phone 1: (283)437-3886x88321\n",
      "Phone 2: 999-728-1637\n",
      "Email: louis27@gilbert.com\n",
      "Subscription Date: 2020-02-25\n",
      "Website: http://www.berger.net/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 5}\n",
      "Content data : \n",
      "Index: 7\n",
      "Customer Id: EA4d384DfDbBf77\n",
      "First Name: Darren\n",
      "Last Name: Peck\n",
      "Company: Lester, Woodard and Mitchell\n",
      "City: Lake Ana\n",
      "Country: Pitcairn Islands\n",
      "Phone 1: (496)452-6181x3291\n",
      "Phone 2: +1-247-266-0963x4995\n",
      "Email: tgates@cantrell.com\n",
      "Subscription Date: 2021-08-24\n",
      "Website: https://www.le.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 6}\n",
      "Content data : \n",
      "Index: 8\n",
      "Customer Id: 0e04AFde9f225dE\n",
      "First Name: Brett\n",
      "Last Name: Mullen\n",
      "Company: Sanford, Davenport and Giles\n",
      "City: Kimport\n",
      "Country: Bulgaria\n",
      "Phone 1: 001-583-352-7197x297\n",
      "Phone 2: 001-333-145-0369\n",
      "Email: asnow@colon.com\n",
      "Subscription Date: 2021-04-12\n",
      "Website: https://hammond-ramsey.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 7}\n",
      "Content data : \n",
      "Index: 9\n",
      "Customer Id: C2dE4dEEc489ae0\n",
      "First Name: Sheryl\n",
      "Last Name: Meyers\n",
      "Company: Browning-Simon\n",
      "City: Robersonstad\n",
      "Country: Cyprus\n",
      "Phone 1: 854-138-4911x5772\n",
      "Phone 2: +1-448-910-2276x729\n",
      "Email: mariokhan@ryan-pope.org\n",
      "Subscription Date: 2020-01-13\n",
      "Website: https://www.bullock.net/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 8}\n",
      "Content data : \n",
      "Index: 10\n",
      "Customer Id: 8C2811a503C7c5a\n",
      "First Name: Michelle\n",
      "Last Name: Gallagher\n",
      "Company: Beck-Hendrix\n",
      "City: Elaineberg\n",
      "Country: Timor-Leste\n",
      "Phone 1: 739.218.2516x459\n",
      "Phone 2: 001-054-401-0347x617\n",
      "Email: mdyer@escobar.net\n",
      "Subscription Date: 2021-11-08\n",
      "Website: https://arias.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 9}\n",
      "Content data : \n",
      "Index: 11\n",
      "Customer Id: 216E205d6eBb815\n",
      "First Name: Carl\n",
      "Last Name: Schroeder\n",
      "Company: Oconnell, Meza and Everett\n",
      "City: Shannonville\n",
      "Country: Guernsey\n",
      "Phone 1: 637-854-0256x825\n",
      "Phone 2: 114.336.0784x788\n",
      "Email: kirksalas@webb.com\n",
      "Subscription Date: 2021-10-20\n",
      "Website: https://simmons-hurley.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 10}\n",
      "Content data : \n",
      "Index: 12\n",
      "Customer Id: CEDec94deE6d69B\n",
      "First Name: Jenna\n",
      "Last Name: Dodson\n",
      "Company: Hoffman, Reed and Mcclain\n",
      "City: East Andrea\n",
      "Country: Vietnam\n",
      "Phone 1: (041)737-3846\n",
      "Phone 2: +1-556-888-3485x42608\n",
      "Email: mark42@robbins.com\n",
      "Subscription Date: 2020-11-29\n",
      "Website: http://www.douglas.net/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 11}\n",
      "Content data : \n",
      "Index: 13\n",
      "Customer Id: e35426EbDEceaFF\n",
      "First Name: Tracey\n",
      "Last Name: Mata\n",
      "Company: Graham-Francis\n",
      "City: South Joannamouth\n",
      "Country: Togo\n",
      "Phone 1: 001-949-844-8787\n",
      "Phone 2: (855)713-8773\n",
      "Email: alex56@walls.org\n",
      "Subscription Date: 2021-12-02\n",
      "Website: http://www.beck.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 12}\n",
      "Content data : \n",
      "Index: 14\n",
      "Customer Id: A08A8aF8BE9FaD4\n",
      "First Name: Kristine\n",
      "Last Name: Cox\n",
      "Company: Carpenter-Cook\n",
      "City: Jodyberg\n",
      "Country: Sri Lanka\n",
      "Phone 1: 786-284-3358x62152\n",
      "Phone 2: +1-315-627-1796x8074\n",
      "Email: holdenmiranda@clarke.com\n",
      "Subscription Date: 2021-02-08\n",
      "Website: https://www.brandt.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 13}\n",
      "Content data : \n",
      "Index: 15\n",
      "Customer Id: 6fEaA1b7cab7B6C\n",
      "First Name: Faith\n",
      "Last Name: Lutz\n",
      "Company: Carter-Hancock\n",
      "City: Burchbury\n",
      "Country: Singapore\n",
      "Phone 1: (781)861-7180x8306\n",
      "Phone 2: 207-185-3665\n",
      "Email: cassieparrish@blevins-chapman.net\n",
      "Subscription Date: 2022-01-26\n",
      "Website: http://stevenson.org/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 14}\n",
      "Content data : \n",
      "Index: 16\n",
      "Customer Id: 8cad0b4CBceaeec\n",
      "First Name: Miranda\n",
      "Last Name: Beasley\n",
      "Company: Singleton and Sons\n",
      "City: Desireeshire\n",
      "Country: Oman\n",
      "Phone 1: 540.085.3135x185\n",
      "Phone 2: +1-600-462-6432x21881\n",
      "Email: vduncan@parks-hardy.com\n",
      "Subscription Date: 2022-04-12\n",
      "Website: http://acosta.org/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 15}\n",
      "Content data : \n",
      "Index: 17\n",
      "Customer Id: a5DC21AE3a21eaA\n",
      "First Name: Caroline\n",
      "Last Name: Foley\n",
      "Company: Winters-Mendoza\n",
      "City: West Adriennestad\n",
      "Country: Western Sahara\n",
      "Phone 1: 936.222.4746x9924\n",
      "Phone 2: 001-469-948-6341x359\n",
      "Email: holtgwendolyn@watson-davenport.com\n",
      "Subscription Date: 2021-03-10\n",
      "Website: http://www.benson-roth.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 16}\n",
      "Content data : \n",
      "Index: 18\n",
      "Customer Id: F8Aa9d6DfcBeeF8\n",
      "First Name: Greg\n",
      "Last Name: Mata\n",
      "Company: Valentine LLC\n",
      "City: Lake Leslie\n",
      "Country: Mozambique\n",
      "Phone 1: (701)087-2415\n",
      "Phone 2: (195)156-1861x26241\n",
      "Email: jaredjuarez@carroll.org\n",
      "Subscription Date: 2022-03-26\n",
      "Website: http://pitts-cherry.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 17}\n",
      "Content data : \n",
      "Index: 19\n",
      "Customer Id: F160f5Db3EfE973\n",
      "First Name: Clifford\n",
      "Last Name: Jacobson\n",
      "Company: Simon LLC\n",
      "City: Harmonview\n",
      "Country: South Georgia and the South Sandwich Islands\n",
      "Phone 1: 001-151-330-3524x0469\n",
      "Phone 2: (748)477-7174\n",
      "Email: joseph26@jacobson.com\n",
      "Subscription Date: 2020-09-24\n",
      "Website: https://mcconnell.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 18}\n",
      "Content data : \n",
      "Index: 20\n",
      "Customer Id: 0F60FF3DdCd7aB0\n",
      "First Name: Joanna\n",
      "Last Name: Kirk\n",
      "Company: Mays-Mccormick\n",
      "City: Jamesshire\n",
      "Country: French Polynesia\n",
      "Phone 1: (266)131-7001x711\n",
      "Phone 2: (283)312-5579x11543\n",
      "Email: tuckerangie@salazar.net\n",
      "Subscription Date: 2021-09-24\n",
      "Website: https://www.camacho.net/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 19}\n",
      "Content data : \n",
      "Index: 21\n",
      "Customer Id: 9F9AdB7B8A6f7F2\n",
      "First Name: Maxwell\n",
      "Last Name: Frye\n",
      "Company: Patterson Inc\n",
      "City: East Carly\n",
      "Country: Malta\n",
      "Phone 1: 423.262.3059\n",
      "Phone 2: 202-880-0688x7491\n",
      "Email: fgibson@drake-webb.com\n",
      "Subscription Date: 2022-01-12\n",
      "Website: http://www.roberts.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 20}\n",
      "Content data : \n",
      "Index: 22\n",
      "Customer Id: FBd0Ded4F02a742\n",
      "First Name: Kiara\n",
      "Last Name: Houston\n",
      "Company: Manning, Hester and Arroyo\n",
      "City: South Alvin\n",
      "Country: Netherlands\n",
      "Phone 1: 001-274-040-3582x10611\n",
      "Phone 2: +1-528-175-0973x4684\n",
      "Email: blanchardbob@wallace-shannon.com\n",
      "Subscription Date: 2020-09-15\n",
      "Website: https://www.reid-potts.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 21}\n",
      "Content data : \n",
      "Index: 23\n",
      "Customer Id: 2FB0FAA1d429421\n",
      "First Name: Colleen\n",
      "Last Name: Howard\n",
      "Company: Greer and Sons\n",
      "City: Brittanyview\n",
      "Country: Paraguay\n",
      "Phone 1: 1935085151\n",
      "Phone 2: (947)115-7711x5488\n",
      "Email: rsingleton@ryan-cherry.com\n",
      "Subscription Date: 2020-08-19\n",
      "Website: http://paul.biz/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 22}\n",
      "Content data : \n",
      "Index: 24\n",
      "Customer Id: 010468dAA11382c\n",
      "First Name: Janet\n",
      "Last Name: Valenzuela\n",
      "Company: Watts-Donaldson\n",
      "City: Veronicamouth\n",
      "Country: Lao People's Democratic Republic\n",
      "Phone 1: 354.259.5062x7538\n",
      "Phone 2: 500.433.2022\n",
      "Email: stefanie71@spence.com\n",
      "Subscription Date: 2020-09-08\n",
      "Website: https://moreno.biz/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 23}\n",
      "Content data : \n",
      "Index: 25\n",
      "Customer Id: eC1927Ca84E033e\n",
      "First Name: Shane\n",
      "Last Name: Wilcox\n",
      "Company: Tucker LLC\n",
      "City: Bryanville\n",
      "Country: Albania\n",
      "Phone 1: (429)005-9030x11004\n",
      "Phone 2: 541-116-4501\n",
      "Email: mariah88@santos.com\n",
      "Subscription Date: 2021-04-06\n",
      "Website: https://www.ramos.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 24}\n",
      "Content data : \n",
      "Index: 26\n",
      "Customer Id: 09D7D7C8Fe09aea\n",
      "First Name: Marcus\n",
      "Last Name: Moody\n",
      "Company: Giles Ltd\n",
      "City: Kaitlyntown\n",
      "Country: Panama\n",
      "Phone 1: 674-677-8623\n",
      "Phone 2: 909-277-5485x566\n",
      "Email: donnamullins@norris-barrett.org\n",
      "Subscription Date: 2022-05-24\n",
      "Website: https://www.curry.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 25}\n",
      "Content data : \n",
      "Index: 27\n",
      "Customer Id: aBdfcF2c50b0bfD\n",
      "First Name: Dakota\n",
      "Last Name: Poole\n",
      "Company: Simmons Group\n",
      "City: Michealshire\n",
      "Country: Belarus\n",
      "Phone 1: (371)987-8576x4720\n",
      "Phone 2: 071-152-1376\n",
      "Email: stacey67@fields.org\n",
      "Subscription Date: 2022-02-20\n",
      "Website: https://sanford-wilcox.biz/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 26}\n",
      "Content data : \n",
      "Index: 28\n",
      "Customer Id: b92EBfdF8a3f0E6\n",
      "First Name: Frederick\n",
      "Last Name: Harper\n",
      "Company: Hinton, Chaney and Stokes\n",
      "City: South Marissatown\n",
      "Country: Switzerland\n",
      "Phone 1: +1-077-121-1558x0687\n",
      "Phone 2: 264.742.7149\n",
      "Email: jacobkhan@bright.biz\n",
      "Subscription Date: 2022-05-26\n",
      "Website: https://callahan.org/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 27}\n",
      "Content data : \n",
      "Index: 29\n",
      "Customer Id: 3B5dAAFA41AFa22\n",
      "First Name: Stefanie\n",
      "Last Name: Fitzpatrick\n",
      "Company: Santana-Duran\n",
      "City: Acevedoville\n",
      "Country: Saint Vincent and the Grenadines\n",
      "Phone 1: (752)776-3286\n",
      "Phone 2: +1-472-021-4814x85074\n",
      "Email: wterrell@clark.com\n",
      "Subscription Date: 2020-07-30\n",
      "Website: https://meyers.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 28}\n",
      "Content data : \n",
      "Index: 30\n",
      "Customer Id: EDA69ca7a6e96a2\n",
      "First Name: Kent\n",
      "Last Name: Bradshaw\n",
      "Company: Sawyer PLC\n",
      "City: North Harold\n",
      "Country: Tanzania\n",
      "Phone 1: +1-472-143-5037x884\n",
      "Phone 2: 126.922.6153\n",
      "Email: qjimenez@boyd.com\n",
      "Subscription Date: 2020-04-26\n",
      "Website: http://maynard-ho.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 29}\n",
      "Content data : \n",
      "Index: 31\n",
      "Customer Id: 64DCcDFaB9DFd4e\n",
      "First Name: Jack\n",
      "Last Name: Tate\n",
      "Company: Acosta, Petersen and Morrow\n",
      "City: West Samuel\n",
      "Country: Zimbabwe\n",
      "Phone 1: 965-108-4406x20714\n",
      "Phone 2: 046.906.1442x6784\n",
      "Email: gfigueroa@boone-zavala.com\n",
      "Subscription Date: 2021-09-15\n",
      "Website: http://www.hawkins-ramsey.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 30}\n",
      "Content data : \n",
      "Index: 32\n",
      "Customer Id: 679c6c83DD872d6\n",
      "First Name: Tom\n",
      "Last Name: Trujillo\n",
      "Company: Mcgee Group\n",
      "City: Cunninghamborough\n",
      "Country: Denmark\n",
      "Phone 1: 416-338-3758\n",
      "Phone 2: (775)890-7209\n",
      "Email: tapiagreg@beard.info\n",
      "Subscription Date: 2022-01-13\n",
      "Website: http://www.daniels-klein.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 31}\n",
      "Content data : \n",
      "Index: 33\n",
      "Customer Id: 7Ce381e4Afa4ba9\n",
      "First Name: Gabriel\n",
      "Last Name: Mejia\n",
      "Company: Adkins-Salinas\n",
      "City: Port Annatown\n",
      "Country: Liechtenstein\n",
      "Phone 1: 4077245425\n",
      "Phone 2: 646.044.0696x66800\n",
      "Email: coleolson@jennings.net\n",
      "Subscription Date: 2021-04-24\n",
      "Website: https://patel-hanson.info/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 32}\n",
      "Content data : \n",
      "Index: 34\n",
      "Customer Id: A09AEc6E3bF70eE\n",
      "First Name: Kaitlyn\n",
      "Last Name: Santana\n",
      "Company: Herrera Group\n",
      "City: New Kaitlyn\n",
      "Country: United States of America\n",
      "Phone 1: 6303643286\n",
      "Phone 2: 447-710-6202x07313\n",
      "Email: georgeross@miles.org\n",
      "Subscription Date: 2021-09-21\n",
      "Website: http://pham.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 33}\n",
      "Content data : \n",
      "Index: 35\n",
      "Customer Id: aA9BAFfBc3710fe\n",
      "First Name: Faith\n",
      "Last Name: Moon\n",
      "Company: Waters, Chase and Aguilar\n",
      "City: West Marthaburgh\n",
      "Country: Bahamas\n",
      "Phone 1: +1-586-217-0359x6317\n",
      "Phone 2: +1-818-199-1403\n",
      "Email: willistonya@randolph-baker.com\n",
      "Subscription Date: 2021-11-03\n",
      "Website: https://spencer-charles.info/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 34}\n",
      "Content data : \n",
      "Index: 36\n",
      "Customer Id: E11dfb2DB8C9f72\n",
      "First Name: Tammie\n",
      "Last Name: Haley\n",
      "Company: Palmer, Barnes and Houston\n",
      "City: East Teresa\n",
      "Country: Belize\n",
      "Phone 1: 001-276-734-4113x6087\n",
      "Phone 2: (430)300-8770\n",
      "Email: harrisisaiah@jenkins.com\n",
      "Subscription Date: 2022-01-04\n",
      "Website: http://evans-simon.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 35}\n",
      "Content data : \n",
      "Index: 37\n",
      "Customer Id: 889eCf90f68c5Da\n",
      "First Name: Nicholas\n",
      "Last Name: Sosa\n",
      "Company: Jordan Ltd\n",
      "City: South Hunter\n",
      "Country: Uruguay\n",
      "Phone 1: (661)425-6042\n",
      "Phone 2: 975-998-1519\n",
      "Email: fwolfe@dorsey.com\n",
      "Subscription Date: 2021-08-10\n",
      "Website: https://www.fleming-richards.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 36}\n",
      "Content data : \n",
      "Index: 38\n",
      "Customer Id: 7a1Ee69F4fF4B4D\n",
      "First Name: Jordan\n",
      "Last Name: Gay\n",
      "Company: Glover and Sons\n",
      "City: South Walter\n",
      "Country: Solomon Islands\n",
      "Phone 1: 7208417020\n",
      "Phone 2: 8035336772\n",
      "Email: tiffanydavies@harris-mcfarland.org\n",
      "Subscription Date: 2021-02-24\n",
      "Website: http://www.lee.org/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 37}\n",
      "Content data : \n",
      "Index: 39\n",
      "Customer Id: dca4f1D0A0fc5c9\n",
      "First Name: Bruce\n",
      "Last Name: Esparza\n",
      "Company: Huerta-Mclean\n",
      "City: Poolefurt\n",
      "Country: Montenegro\n",
      "Phone 1: 559-529-4424\n",
      "Phone 2: 001-625-000-7132x0367\n",
      "Email: preese@frye-vega.com\n",
      "Subscription Date: 2021-10-22\n",
      "Website: http://www.farley.org/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 38}\n",
      "Content data : \n",
      "Index: 40\n",
      "Customer Id: 17aD8e2dB3df03D\n",
      "First Name: Sherry\n",
      "Last Name: Garza\n",
      "Company: Anderson Ltd\n",
      "City: West John\n",
      "Country: Poland\n",
      "Phone 1: 001-067-713-6440x158\n",
      "Phone 2: (978)289-8785x5766\n",
      "Email: ann48@miller.com\n",
      "Subscription Date: 2021-11-01\n",
      "Website: http://spence.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 39}\n",
      "Content data : \n",
      "Index: 41\n",
      "Customer Id: 2f79Cd309624Abb\n",
      "First Name: Natalie\n",
      "Last Name: Gentry\n",
      "Company: Monroe PLC\n",
      "City: West Darius\n",
      "Country: Dominican Republic\n",
      "Phone 1: 830.996.8238\n",
      "Phone 2: 499.122.5415\n",
      "Email: tcummings@fitzpatrick-ashley.com\n",
      "Subscription Date: 2020-10-10\n",
      "Website: http://www.dorsey.biz/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 40}\n",
      "Content data : \n",
      "Index: 42\n",
      "Customer Id: 6e5ad5a5e2bB5Ca\n",
      "First Name: Bryan\n",
      "Last Name: Dunn\n",
      "Company: Kaufman and Sons\n",
      "City: North Jimstad\n",
      "Country: Burkina Faso\n",
      "Phone 1: 001-710-802-5565\n",
      "Phone 2: 078.699.8982x13881\n",
      "Email: woodwardandres@phelps.com\n",
      "Subscription Date: 2021-09-08\n",
      "Website: http://www.butler.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 41}\n",
      "Content data : \n",
      "Index: 43\n",
      "Customer Id: 7E441b6B228DBcA\n",
      "First Name: Wayne\n",
      "Last Name: Simpson\n",
      "Company: Perkins-Trevino\n",
      "City: East Rebekahborough\n",
      "Country: Bolivia\n",
      "Phone 1: (344)156-8632x1869\n",
      "Phone 2: 463-445-3702x38463\n",
      "Email: barbarapittman@holder.com\n",
      "Subscription Date: 2020-12-13\n",
      "Website: https://gillespie-holder.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 42}\n",
      "Content data : \n",
      "Index: 44\n",
      "Customer Id: D3fC11A9C235Dc6\n",
      "First Name: Luis\n",
      "Last Name: Greer\n",
      "Company: Cross PLC\n",
      "City: North Drew\n",
      "Country: Bulgaria\n",
      "Phone 1: 001-336-025-6849x701\n",
      "Phone 2: 684.698.2911x6092\n",
      "Email: bstuart@williamson-mcclure.com\n",
      "Subscription Date: 2022-05-15\n",
      "Website: https://fletcher-nielsen.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 43}\n",
      "Content data : \n",
      "Index: 45\n",
      "Customer Id: 30Dfa48fe5Ede78\n",
      "First Name: Rhonda\n",
      "Last Name: Frost\n",
      "Company: Herrera, Shepherd and Underwood\n",
      "City: Lake Lindaburgh\n",
      "Country: Monaco\n",
      "Phone 1: (127)081-9339\n",
      "Phone 2: +1-431-028-3337x3492\n",
      "Email: zkrueger@wolf-chavez.net\n",
      "Subscription Date: 2021-12-06\n",
      "Website: http://www.khan.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 44}\n",
      "Content data : \n",
      "Index: 46\n",
      "Customer Id: fD780ED8dbEae7B\n",
      "First Name: Joanne\n",
      "Last Name: Montes\n",
      "Company: Price, Sexton and Mcdaniel\n",
      "City: Gwendolynview\n",
      "Country: Palau\n",
      "Phone 1: (897)726-7952\n",
      "Phone 2: (467)886-9467x5721\n",
      "Email: juan80@henson.net\n",
      "Subscription Date: 2020-07-01\n",
      "Website: http://ochoa.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 45}\n",
      "Content data : \n",
      "Index: 47\n",
      "Customer Id: 300A40d3ce24bBA\n",
      "First Name: Geoffrey\n",
      "Last Name: Guzman\n",
      "Company: Short-Wiggins\n",
      "City: Zimmermanland\n",
      "Country: Uzbekistan\n",
      "Phone 1: 975.235.8921x269\n",
      "Phone 2: (983)188-6873\n",
      "Email: bauercrystal@gay.com\n",
      "Subscription Date: 2020-04-23\n",
      "Website: https://decker-kline.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 46}\n",
      "Content data : \n",
      "Index: 48\n",
      "Customer Id: 283DFCD0Dba40aF\n",
      "First Name: Gloria\n",
      "Last Name: Mccall\n",
      "Company: Brennan, Acosta and Ramos\n",
      "City: North Kerriton\n",
      "Country: Ghana\n",
      "Phone 1: 445-603-6729\n",
      "Phone 2: 001-395-959-4736x4524\n",
      "Email: bartlettjenna@zuniga-moss.biz\n",
      "Subscription Date: 2022-03-11\n",
      "Website: http://burgess-frank.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 47}\n",
      "Content data : \n",
      "Index: 49\n",
      "Customer Id: F4Fc91fEAEad286\n",
      "First Name: Brady\n",
      "Last Name: Cohen\n",
      "Company: Osborne-Erickson\n",
      "City: North Eileenville\n",
      "Country: United Arab Emirates\n",
      "Phone 1: 741.849.0139x524\n",
      "Phone 2: +1-028-691-7497x0894\n",
      "Email: mccalltyrone@durham-rose.biz\n",
      "Subscription Date: 2022-03-10\n",
      "Website: http://hammond-barron.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 48}\n",
      "Content data : \n",
      "Index: 50\n",
      "Customer Id: 80F33Fd2AcebF05\n",
      "First Name: Latoya\n",
      "Last Name: Mccann\n",
      "Company: Hobbs, Garrett and Sanford\n",
      "City: Port Sergiofort\n",
      "Country: Belarus\n",
      "Phone 1: (530)287-4548x29481\n",
      "Phone 2: 162-234-0249x32790\n",
      "Email: bobhammond@barry.biz\n",
      "Subscription Date: 2021-12-02\n",
      "Website: https://www.burton.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 49}\n",
      "Content data : \n",
      "Index: 51\n",
      "Customer Id: Aa20BDe68eAb0e9\n",
      "First Name: Gerald\n",
      "Last Name: Hawkins\n",
      "Company: Phelps, Forbes and Koch\n",
      "City: New Alberttown\n",
      "Country: Canada\n",
      "Phone 1: +1-323-239-1456x96168\n",
      "Phone 2: (092)508-0269\n",
      "Email: uwarner@steele-arias.com\n",
      "Subscription Date: 2021-03-19\n",
      "Website: https://valenzuela.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 50}\n",
      "Content data : \n",
      "Index: 52\n",
      "Customer Id: e898eEB1B9FE22b\n",
      "First Name: Samuel\n",
      "Last Name: Crawford\n",
      "Company: May, Goodwin and Martin\n",
      "City: South Jasmine\n",
      "Country: Algeria\n",
      "Phone 1: 802-242-7457\n",
      "Phone 2: 626.116.9535x8578\n",
      "Email: xpittman@ritter-carney.net\n",
      "Subscription Date: 2021-03-27\n",
      "Website: https://guerrero.org/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 51}\n",
      "Content data : \n",
      "Index: 53\n",
      "Customer Id: faCEF517ae7D8eB\n",
      "First Name: Patricia\n",
      "Last Name: Goodwin\n",
      "Company: Christian, Winters and Ellis\n",
      "City: Cowanfort\n",
      "Country: Swaziland\n",
      "Phone 1: 322.549.7139x70040\n",
      "Phone 2: (111)741-4173\n",
      "Email: vaughanchristy@lara.biz\n",
      "Subscription Date: 2021-03-08\n",
      "Website: http://clark.info/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 52}\n",
      "Content data : \n",
      "Index: 54\n",
      "Customer Id: c09952De6Cda8aA\n",
      "First Name: Stacie\n",
      "Last Name: Richard\n",
      "Company: Byrd Inc\n",
      "City: New Deborah\n",
      "Country: Madagascar\n",
      "Phone 1: 001-622-948-3641x24810\n",
      "Phone 2: 001-731-168-2893x8891\n",
      "Email: clinton85@colon-arias.org\n",
      "Subscription Date: 2020-10-15\n",
      "Website: https://kim.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 53}\n",
      "Content data : \n",
      "Index: 55\n",
      "Customer Id: f3BEf3Be028166f\n",
      "First Name: Robin\n",
      "Last Name: West\n",
      "Company: Nixon, Blackwell and Sosa\n",
      "City: Wallstown\n",
      "Country: Ecuador\n",
      "Phone 1: 698.303.4267\n",
      "Phone 2: 001-683-837-7651x525\n",
      "Email: greenemiranda@zimmerman.com\n",
      "Subscription Date: 2022-01-13\n",
      "Website: https://www.mora.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 54}\n",
      "Content data : \n",
      "Index: 56\n",
      "Customer Id: C6F2Fc6a7948a4e\n",
      "First Name: Ralph\n",
      "Last Name: Haas\n",
      "Company: Montes PLC\n",
      "City: Lake Ellenchester\n",
      "Country: Palestinian Territory\n",
      "Phone 1: 2239271999\n",
      "Phone 2: 001-962-434-0867x649\n",
      "Email: goodmancesar@figueroa.biz\n",
      "Subscription Date: 2020-05-25\n",
      "Website: http://may.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 55}\n",
      "Content data : \n",
      "Index: 57\n",
      "Customer Id: c8FE57cBBdCDcb2\n",
      "First Name: Phyllis\n",
      "Last Name: Maldonado\n",
      "Company: Costa PLC\n",
      "City: Lake Whitney\n",
      "Country: Saint Barthelemy\n",
      "Phone 1: 4500370767\n",
      "Phone 2: 001-508-064-6725x017\n",
      "Email: yhanson@warner-diaz.org\n",
      "Subscription Date: 2021-01-25\n",
      "Website: http://www.bernard.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 56}\n",
      "Content data : \n",
      "Index: 58\n",
      "Customer Id: B5acdFC982124F2\n",
      "First Name: Danny\n",
      "Last Name: Parrish\n",
      "Company: Novak LLC\n",
      "City: East Jaredbury\n",
      "Country: United Arab Emirates\n",
      "Phone 1: (669)384-8597x8794\n",
      "Phone 2: 506.731.5952x571\n",
      "Email: howelldarren@house-cohen.com\n",
      "Subscription Date: 2021-03-17\n",
      "Website: http://www.parsons-hudson.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 57}\n",
      "Content data : \n",
      "Index: 59\n",
      "Customer Id: 8c7DdF10798bCC3\n",
      "First Name: Kathy\n",
      "Last Name: Hill\n",
      "Company: Moore, Mccoy and Glass\n",
      "City: Selenabury\n",
      "Country: South Georgia and the South Sandwich Islands\n",
      "Phone 1: 001-171-716-2175x310\n",
      "Phone 2: 888.625.0654\n",
      "Email: ncamacho@boone-simmons.org\n",
      "Subscription Date: 2020-11-15\n",
      "Website: http://hayden.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 58}\n",
      "Content data : \n",
      "Index: 60\n",
      "Customer Id: C681dDd0cc422f7\n",
      "First Name: Kelli\n",
      "Last Name: Hardy\n",
      "Company: Petty Ltd\n",
      "City: Huangfort\n",
      "Country: Sao Tome and Principe\n",
      "Phone 1: 020.324.2191x2022\n",
      "Phone 2: 424-157-8216\n",
      "Email: kristopher62@oliver.com\n",
      "Subscription Date: 2020-12-20\n",
      "Website: http://www.kidd.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 59}\n",
      "Content data : \n",
      "Index: 61\n",
      "Customer Id: a940cE42e035F28\n",
      "First Name: Lynn\n",
      "Last Name: Pham\n",
      "Company: Brennan, Camacho and Tapia\n",
      "City: East Pennyshire\n",
      "Country: Portugal\n",
      "Phone 1: 846.468.6834x611\n",
      "Phone 2: 001-248-691-0006\n",
      "Email: mpham@rios-guzman.com\n",
      "Subscription Date: 2020-08-21\n",
      "Website: https://www.murphy.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 60}\n",
      "Content data : \n",
      "Index: 62\n",
      "Customer Id: 9Cf5E6AFE0aeBfd\n",
      "First Name: Shelley\n",
      "Last Name: Harris\n",
      "Company: Prince, Malone and Pugh\n",
      "City: Port Jasminborough\n",
      "Country: Togo\n",
      "Phone 1: 423.098.0315x8373\n",
      "Phone 2: +1-386-458-8944x15194\n",
      "Email: zachary96@mitchell-bryant.org\n",
      "Subscription Date: 2020-12-10\n",
      "Website: https://www.ryan.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 61}\n",
      "Content data : \n",
      "Index: 63\n",
      "Customer Id: aEcbe5365BbC67D\n",
      "First Name: Eddie\n",
      "Last Name: Jimenez\n",
      "Company: Caldwell Group\n",
      "City: West Kristine\n",
      "Country: Ethiopia\n",
      "Phone 1: +1-235-657-1073x6306\n",
      "Phone 2: (026)401-7353x2417\n",
      "Email: kristiwhitney@bernard.com\n",
      "Subscription Date: 2022-03-24\n",
      "Website: http://cherry.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 62}\n",
      "Content data : \n",
      "Index: 64\n",
      "Customer Id: FCBdfCEAe20A8Dc\n",
      "First Name: Chloe\n",
      "Last Name: Hutchinson\n",
      "Company: Simon LLC\n",
      "City: South Julia\n",
      "Country: Netherlands\n",
      "Phone 1: 981-544-9452\n",
      "Phone 2: +1-288-552-4666x060\n",
      "Email: leah85@sutton-terrell.com\n",
      "Subscription Date: 2022-05-15\n",
      "Website: https://mitchell.info/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 63}\n",
      "Content data : \n",
      "Index: 65\n",
      "Customer Id: 636cBF0835E10ff\n",
      "First Name: Eileen\n",
      "Last Name: Lynch\n",
      "Company: Knight, Abbott and Hubbard\n",
      "City: Helenborough\n",
      "Country: Liberia\n",
      "Phone 1: +1-158-951-4131x53578\n",
      "Phone 2: 001-673-779-6713x680\n",
      "Email: levigiles@vincent.com\n",
      "Subscription Date: 2021-01-02\n",
      "Website: http://mckay.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 64}\n",
      "Content data : \n",
      "Index: 66\n",
      "Customer Id: fF1b6c9E8Fbf1ff\n",
      "First Name: Fernando\n",
      "Last Name: Lambert\n",
      "Company: Church-Banks\n",
      "City: Lake Nancy\n",
      "Country: Lithuania\n",
      "Phone 1: 497.829.9038\n",
      "Phone 2: 3863743398\n",
      "Email: fisherlinda@schaefer.net\n",
      "Subscription Date: 2021-04-23\n",
      "Website: https://www.vang.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 65}\n",
      "Content data : \n",
      "Index: 67\n",
      "Customer Id: 2A13F74EAa7DA6c\n",
      "First Name: Makayla\n",
      "Last Name: Cannon\n",
      "Company: Henderson Inc\n",
      "City: Georgeport\n",
      "Country: New Caledonia\n",
      "Phone 1: 001-215-801-6392x46009\n",
      "Phone 2: 027-609-6460\n",
      "Email: scottcurtis@hurley.biz\n",
      "Subscription Date: 2020-01-20\n",
      "Website: http://www.velazquez.net/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 66}\n",
      "Content data : \n",
      "Index: 68\n",
      "Customer Id: a014Ec1b9FccC1E\n",
      "First Name: Tom\n",
      "Last Name: Alvarado\n",
      "Company: Donaldson-Dougherty\n",
      "City: South Sophiaberg\n",
      "Country: Kiribati\n",
      "Phone 1: (585)606-2980x2258\n",
      "Phone 2: 730-797-3594x5614\n",
      "Email: nicholsonnina@montgomery.info\n",
      "Subscription Date: 2020-08-18\n",
      "Website: http://odom-massey.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 67}\n",
      "Content data : \n",
      "Index: 69\n",
      "Customer Id: 421a109cABDf5fa\n",
      "First Name: Virginia\n",
      "Last Name: Dudley\n",
      "Company: Warren Ltd\n",
      "City: Hartbury\n",
      "Country: French Southern Territories\n",
      "Phone 1: 027.846.3705x14184\n",
      "Phone 2: +1-439-171-1846x4636\n",
      "Email: zvalencia@phelps.com\n",
      "Subscription Date: 2021-01-31\n",
      "Website: http://hunter-esparza.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 68}\n",
      "Content data : \n",
      "Index: 70\n",
      "Customer Id: CC68FD1D3Bbbf22\n",
      "First Name: Riley\n",
      "Last Name: Good\n",
      "Company: Wade PLC\n",
      "City: Erikaville\n",
      "Country: Canada\n",
      "Phone 1: 6977745822\n",
      "Phone 2: 855-436-7641\n",
      "Email: alex06@galloway.com\n",
      "Subscription Date: 2020-02-03\n",
      "Website: http://conway.org/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 69}\n",
      "Content data : \n",
      "Index: 71\n",
      "Customer Id: CBCd2Ac8E3eBDF9\n",
      "First Name: Alexandria\n",
      "Last Name: Buck\n",
      "Company: Keller-Coffey\n",
      "City: Nicolasfort\n",
      "Country: Iran\n",
      "Phone 1: 078-900-4760x76668\n",
      "Phone 2: 414-112-8700x68751\n",
      "Email: lee48@manning.com\n",
      "Subscription Date: 2021-02-20\n",
      "Website: https://ramsey.org/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 70}\n",
      "Content data : \n",
      "Index: 72\n",
      "Customer Id: Ef859092FbEcC07\n",
      "First Name: Richard\n",
      "Last Name: Roth\n",
      "Company: Conway-Mcbride\n",
      "City: New Jasmineshire\n",
      "Country: Morocco\n",
      "Phone 1: 581-440-6539\n",
      "Phone 2: 9857827463\n",
      "Email: aharper@maddox-townsend.org\n",
      "Subscription Date: 2020-02-23\n",
      "Website: https://www.brooks.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 71}\n",
      "Content data : \n",
      "Index: 73\n",
      "Customer Id: F560f2d3cDFb618\n",
      "First Name: Candice\n",
      "Last Name: Keller\n",
      "Company: Huynh and Sons\n",
      "City: East Summerstad\n",
      "Country: Zimbabwe\n",
      "Phone 1: 001-927-965-8550x92406\n",
      "Phone 2: 001-243-038-4271x53076\n",
      "Email: buckleycory@odonnell.net\n",
      "Subscription Date: 2020-08-22\n",
      "Website: https://www.lucero.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 72}\n",
      "Content data : \n",
      "Index: 74\n",
      "Customer Id: A3F76Be153Df4a3\n",
      "First Name: Anita\n",
      "Last Name: Benson\n",
      "Company: Parrish Ltd\n",
      "City: Skinnerport\n",
      "Country: Russian Federation\n",
      "Phone 1: 874.617.5668x69878\n",
      "Phone 2: (399)820-6418x0071\n",
      "Email: angie04@oconnell.com\n",
      "Subscription Date: 2020-02-09\n",
      "Website: http://oconnor.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 73}\n",
      "Content data : \n",
      "Index: 75\n",
      "Customer Id: D01Af0AF7cBbFeA\n",
      "First Name: Regina\n",
      "Last Name: Stein\n",
      "Company: Guzman-Brown\n",
      "City: Raystad\n",
      "Country: Solomon Islands\n",
      "Phone 1: 001-469-848-0724x4407\n",
      "Phone 2: 001-085-360-4426x00357\n",
      "Email: zrosario@rojas-hardin.net\n",
      "Subscription Date: 2022-01-15\n",
      "Website: http://www.johnston.info/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 74}\n",
      "Content data : \n",
      "Index: 76\n",
      "Customer Id: d40e89dCade7b2F\n",
      "First Name: Debra\n",
      "Last Name: Riddle\n",
      "Company: Chang, Aguirre and Leblanc\n",
      "City: Colinhaven\n",
      "Country: United States Virgin Islands\n",
      "Phone 1: +1-768-182-6014x14336\n",
      "Phone 2: (303)961-4491\n",
      "Email: shieldskerry@robles.com\n",
      "Subscription Date: 2020-07-11\n",
      "Website: http://kaiser.info/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 75}\n",
      "Content data : \n",
      "Index: 77\n",
      "Customer Id: BF6a1f9bd1bf8DE\n",
      "First Name: Brittany\n",
      "Last Name: Zuniga\n",
      "Company: Mason-Hester\n",
      "City: West Reginald\n",
      "Country: Kyrgyz Republic\n",
      "Phone 1: (050)136-9025\n",
      "Phone 2: 001-480-851-2496x0157\n",
      "Email: mchandler@cochran-huerta.org\n",
      "Subscription Date: 2021-07-24\n",
      "Website: http://www.boyle.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 76}\n",
      "Content data : \n",
      "Index: 78\n",
      "Customer Id: FfaeFFbbbf280db\n",
      "First Name: Cassidy\n",
      "Last Name: Mcmahon\n",
      "Company: Mcguire, Huynh and Hopkins\n",
      "City: Lake Sherryborough\n",
      "Country: Myanmar\n",
      "Phone 1: 5040771311\n",
      "Phone 2: 684-682-0021x1326\n",
      "Email: katrinalane@fitzgerald.com\n",
      "Subscription Date: 2020-10-21\n",
      "Website: https://hurst.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 77}\n",
      "Content data : \n",
      "Index: 79\n",
      "Customer Id: CbAE1d1e9a8dCb1\n",
      "First Name: Laurie\n",
      "Last Name: Pennington\n",
      "Company: Sanchez, Marsh and Hale\n",
      "City: Port Katherineville\n",
      "Country: Dominica\n",
      "Phone 1: 007.155.3406x553\n",
      "Phone 2: +1-809-862-5566x277\n",
      "Email: cookejill@powell.com\n",
      "Subscription Date: 2020-06-08\n",
      "Website: http://www.hebert.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 78}\n",
      "Content data : \n",
      "Index: 80\n",
      "Customer Id: A7F85c1DE4dB87f\n",
      "First Name: Alejandro\n",
      "Last Name: Blair\n",
      "Company: Combs, Waller and Durham\n",
      "City: Thomasland\n",
      "Country: Iceland\n",
      "Phone 1: (690)068-4641x51468\n",
      "Phone 2: 555.509.8691x2329\n",
      "Email: elizabethbarr@ewing.com\n",
      "Subscription Date: 2020-09-19\n",
      "Website: https://mercado-blevins.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 79}\n",
      "Content data : \n",
      "Index: 81\n",
      "Customer Id: D6CEAfb3BDbaa1A\n",
      "First Name: Leslie\n",
      "Last Name: Jennings\n",
      "Company: Blankenship-Arias\n",
      "City: Coreybury\n",
      "Country: Micronesia\n",
      "Phone 1: 629.198.6346\n",
      "Phone 2: 075.256.0829\n",
      "Email: corey75@wiggins.com\n",
      "Subscription Date: 2021-11-13\n",
      "Website: https://www.juarez.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 80}\n",
      "Content data : \n",
      "Index: 82\n",
      "Customer Id: Ebdb6F6F7c90b69\n",
      "First Name: Kathleen\n",
      "Last Name: Mckay\n",
      "Company: Coffey, Lamb and Johnson\n",
      "City: Lake Janiceton\n",
      "Country: Saint Vincent and the Grenadines\n",
      "Phone 1: (733)910-9968\n",
      "Phone 2: (691)247-4128x0665\n",
      "Email: chloelester@higgins-wilkinson.com\n",
      "Subscription Date: 2021-09-12\n",
      "Website: http://www.owens-mooney.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 81}\n",
      "Content data : \n",
      "Index: 83\n",
      "Customer Id: E8E7e8Cfe516ef0\n",
      "First Name: Hunter\n",
      "Last Name: Moreno\n",
      "Company: Fitzpatrick-Lawrence\n",
      "City: East Clinton\n",
      "Country: Isle of Man\n",
      "Phone 1: (733)833-6754\n",
      "Phone 2: 001-761-013-7121\n",
      "Email: isaac26@benton-finley.com\n",
      "Subscription Date: 2020-12-28\n",
      "Website: http://walls.info/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 82}\n",
      "Content data : \n",
      "Index: 84\n",
      "Customer Id: 78C06E9b6B3DF20\n",
      "First Name: Chad\n",
      "Last Name: Davidson\n",
      "Company: Garcia-Jimenez\n",
      "City: South Joshuashire\n",
      "Country: Oman\n",
      "Phone 1: 8275702958\n",
      "Phone 2: (804)842-4715\n",
      "Email: justinwalters@jimenez.com\n",
      "Subscription Date: 2021-11-15\n",
      "Website: http://www.garner-oliver.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 83}\n",
      "Content data : \n",
      "Index: 85\n",
      "Customer Id: 03A1E62ADdeb31c\n",
      "First Name: Corey\n",
      "Last Name: Holt\n",
      "Company: Mcdonald, Bird and Ramirez\n",
      "City: New Glenda\n",
      "Country: Fiji\n",
      "Phone 1: 001-439-242-4986x7918\n",
      "Phone 2: 3162708934\n",
      "Email: maurice46@morgan.com\n",
      "Subscription Date: 2020-02-18\n",
      "Website: http://www.watson.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 84}\n",
      "Content data : \n",
      "Index: 86\n",
      "Customer Id: C6763c99d0bd16D\n",
      "First Name: Emma\n",
      "Last Name: Cunningham\n",
      "Company: Stephens Inc\n",
      "City: North Jillianview\n",
      "Country: New Zealand\n",
      "Phone 1: 128-059-0206x60217\n",
      "Phone 2: (312)164-4545x2284\n",
      "Email: walter83@juarez.org\n",
      "Subscription Date: 2022-05-13\n",
      "Website: http://www.reid.info/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 85}\n",
      "Content data : \n",
      "Index: 87\n",
      "Customer Id: ebe77E5Bf9476CE\n",
      "First Name: Duane\n",
      "Last Name: Woods\n",
      "Company: Montoya-Miller\n",
      "City: Lyonsberg\n",
      "Country: Maldives\n",
      "Phone 1: (636)544-7783x7288\n",
      "Phone 2: (203)287-1003x5932\n",
      "Email: kmercer@wagner.com\n",
      "Subscription Date: 2020-07-21\n",
      "Website: http://murray.org/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 86}\n",
      "Content data : \n",
      "Index: 88\n",
      "Customer Id: E4Bbcd8AD81fC5f\n",
      "First Name: Alison\n",
      "Last Name: Vargas\n",
      "Company: Vaughn, Watts and Leach\n",
      "City: East Cristinabury\n",
      "Country: Benin\n",
      "Phone 1: 365-273-8144\n",
      "Phone 2: 053-308-7653x6287\n",
      "Email: vcantu@norton.com\n",
      "Subscription Date: 2020-11-10\n",
      "Website: http://mason.info/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 87}\n",
      "Content data : \n",
      "Index: 89\n",
      "Customer Id: efeb73245CDf1fF\n",
      "First Name: Vernon\n",
      "Last Name: Kane\n",
      "Company: Carter-Strickland\n",
      "City: Thomasfurt\n",
      "Country: Yemen\n",
      "Phone 1: 114-854-1159x555\n",
      "Phone 2: 499-608-4612\n",
      "Email: hilljesse@barrett.info\n",
      "Subscription Date: 2021-04-15\n",
      "Website: http://www.duffy-hensley.net/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 88}\n",
      "Content data : \n",
      "Index: 90\n",
      "Customer Id: 37Ec4B395641c1E\n",
      "First Name: Lori\n",
      "Last Name: Flowers\n",
      "Company: Decker-Mcknight\n",
      "City: North Joeburgh\n",
      "Country: Namibia\n",
      "Phone 1: 679.415.1210\n",
      "Phone 2: 945-842-3659x4581\n",
      "Email: tyrone77@valenzuela.info\n",
      "Subscription Date: 2021-01-09\n",
      "Website: http://www.deleon-crosby.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 89}\n",
      "Content data : \n",
      "Index: 91\n",
      "Customer Id: 5ef6d3eefdD43bE\n",
      "First Name: Nina\n",
      "Last Name: Chavez\n",
      "Company: Byrd-Campbell\n",
      "City: Cassidychester\n",
      "Country: Bhutan\n",
      "Phone 1: 053-344-3205\n",
      "Phone 2: +1-330-920-5422x571\n",
      "Email: elliserica@frank.com\n",
      "Subscription Date: 2020-03-26\n",
      "Website: https://www.pugh.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 90}\n",
      "Content data : \n",
      "Index: 92\n",
      "Customer Id: 98b3aeDcC3B9FF3\n",
      "First Name: Shane\n",
      "Last Name: Foley\n",
      "Company: Rocha-Hart\n",
      "City: South Dannymouth\n",
      "Country: Hungary\n",
      "Phone 1: +1-822-569-0302\n",
      "Phone 2: 001-626-114-5844x55073\n",
      "Email: nsteele@sparks.com\n",
      "Subscription Date: 2021-07-06\n",
      "Website: https://www.holt-sparks.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 91}\n",
      "Content data : \n",
      "Index: 93\n",
      "Customer Id: aAb6AFc7AfD0fF3\n",
      "First Name: Collin\n",
      "Last Name: Ayers\n",
      "Company: Lamb-Peterson\n",
      "City: South Lonnie\n",
      "Country: Anguilla\n",
      "Phone 1: 404-645-5351x012\n",
      "Phone 2: 001-257-582-8850x8516\n",
      "Email: dudleyemily@gonzales.biz\n",
      "Subscription Date: 2021-06-29\n",
      "Website: http://www.ruiz.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 92}\n",
      "Content data : \n",
      "Index: 94\n",
      "Customer Id: 54B5B5Fe9F1B6C5\n",
      "First Name: Sherry\n",
      "Last Name: Young\n",
      "Company: Lee, Lucero and Johnson\n",
      "City: Frankchester\n",
      "Country: Solomon Islands\n",
      "Phone 1: 158-687-1764\n",
      "Phone 2: (438)375-6207x003\n",
      "Email: alan79@gates-mclaughlin.com\n",
      "Subscription Date: 2021-04-04\n",
      "Website: https://travis.net/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 93}\n",
      "Content data : \n",
      "Index: 95\n",
      "Customer Id: BE91A0bdcA49Bbc\n",
      "First Name: Darrell\n",
      "Last Name: Douglas\n",
      "Company: Newton, Petersen and Mathis\n",
      "City: Daisyborough\n",
      "Country: Mali\n",
      "Phone 1: 001-084-845-9524x1777\n",
      "Phone 2: 001-769-564-6303\n",
      "Email: grayjean@lowery-good.com\n",
      "Subscription Date: 2022-02-17\n",
      "Website: https://banks.biz/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 94}\n",
      "Content data : \n",
      "Index: 96\n",
      "Customer Id: cb8E23e48d22Eae\n",
      "First Name: Karl\n",
      "Last Name: Greer\n",
      "Company: Carey LLC\n",
      "City: East Richard\n",
      "Country: Guyana\n",
      "Phone 1: (188)169-1674x58692\n",
      "Phone 2: 001-841-293-3519x614\n",
      "Email: hhart@jensen.com\n",
      "Subscription Date: 2022-01-30\n",
      "Website: http://hayes-perez.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 95}\n",
      "Content data : \n",
      "Index: 97\n",
      "Customer Id: CeD220bdAaCfaDf\n",
      "First Name: Lynn\n",
      "Last Name: Atkinson\n",
      "Company: Ware, Burns and Oneal\n",
      "City: New Bradview\n",
      "Country: Sri Lanka\n",
      "Phone 1: +1-846-706-2218\n",
      "Phone 2: 605.413.3198\n",
      "Email: vkemp@ferrell.com\n",
      "Subscription Date: 2021-07-10\n",
      "Website: https://novak-allison.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 96}\n",
      "Content data : \n",
      "Index: 98\n",
      "Customer Id: 28CDbC0dFe4b1Db\n",
      "First Name: Fred\n",
      "Last Name: Guerra\n",
      "Company: Schmitt-Jones\n",
      "City: Ortegaland\n",
      "Country: Solomon Islands\n",
      "Phone 1: +1-753-067-8419x7170\n",
      "Phone 2: +1-632-666-7507x92121\n",
      "Email: swagner@kane.org\n",
      "Subscription Date: 2021-09-18\n",
      "Website: https://www.ross.com/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 97}\n",
      "Content data : \n",
      "Index: 99\n",
      "Customer Id: c23d1D9EE8DEB0A\n",
      "First Name: Yvonne\n",
      "Last Name: Farmer\n",
      "Company: Fitzgerald-Harrell\n",
      "City: Lake Elijahview\n",
      "Country: Aruba\n",
      "Phone 1: (530)311-9786\n",
      "Phone 2: 001-869-452-0943x12424\n",
      "Email: mccarthystephen@horn-green.biz\n",
      "Subscription Date: 2021-08-11\n",
      "Website: http://watkins.info/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 98}\n",
      "Content data : \n",
      "Index: 100\n",
      "Customer Id: 2354a0E336A91A1\n",
      "First Name: Clarence\n",
      "Last Name: Haynes\n",
      "Company: Le, Nash and Cross\n",
      "City: Judymouth\n",
      "Country: Honduras\n",
      "Phone 1: (753)813-6941\n",
      "Phone 2: 783.639.1472\n",
      "Email: colleen91@faulkner.biz\n",
      "Subscription Date: 2020-03-11\n",
      "Website: http://www.hatfield-saunders.net/\n",
      "Metadata : \n",
      "{'source': 'customers-100.csv', 'row': 99}\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "\n",
    "loader = CSVLoader('customers-100.csv') # Create a loader from a CSV file\n",
    "documents = loader.load() # Load documents\n",
    "\n",
    "# Get the content and metadata of each document\n",
    "for document in documents:\n",
    "    content = document.page_content\n",
    "    metadata = document.metadata\n",
    "    print(\"Content data : \")\n",
    "    print(content)\n",
    "    print(\"Metadata : \")\n",
    "    print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The are some loaders developed for specific public places, like `Wikipedia`. In that case, you can use the `WikipediaLoader` object.\n",
    "\n",
    "Another important loader is `UnstructuredUrlLoader`, which allows to get information from public web pages. That tool would be interesting for web scratching. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'title': 'Machine learning', 'summary': 'Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Quick progress in the field of deep learning, beginning in 2010s, allowed neural networks to surpass many previous approaches in performance.\\nML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics.\\nStatistics and mathematical optimization (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning. \\nFrom a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Machine_learning'}, page_content='Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Quick progress in the field of deep learning, beginning in 2010s, allowed neural networks to surpass many previous approaches in performance.\\nML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics.\\nStatistics and mathematical optimization (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning. \\nFrom a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.\\n\\n\\n== History ==\\n\\nThe term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period.\\nAlthough the earliest machine learning model was introduced in the 1950s when Arthur Samuel invented a program that calculated the winning chance in checkers for each side, the history of machine learning roots back to decades of human desire and effort to study human cognitive processes. In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. Hebb\\'s model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data. Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.\\nBy the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyze sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognize patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nilsson\\'s book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981 a report was given on using teaching strategies so that an artificial neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.\\nTom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing\\'s proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\".\\nModern-day machine learning has two objectives.  One is to classify data based on models which have been developed; the other purpose is to make predictions for future outcomes based on thes'), Document(metadata={'title': 'Attention (machine learning)', 'summary': 'Attention is a machine learning method that determines the relative importance of each component in a sequence relative to the other components in that sequence. In natural language processing, importance is represented by \"soft\" weights assigned to each word in a sentence. More generally, attention encodes vectors called token embeddings across a fixed-width sequence that can range from tens to millions of tokens in size.\\nUnlike \"hard\" weights, which are computed during the backwards training pass, \"soft\" weights exist only in the forward pass and therefore change with every step of the input. Earlier designs implemented the attention mechanism in a serial recurrent neural network language translation system, but the later transformer design removed the slower sequential RNN and relied more heavily on the faster parallel attention scheme.\\nInspired by ideas about attention in humans, the attention mechanism was developed to address the weaknesses of leveraging information from the hidden layers of recurrent neural networks. Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence tends to be attenuated. Attention allows a token equal access to any part of a sentence directly, rather than only through the previous state.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Attention_(machine_learning)'}, page_content='Attention is a machine learning method that determines the relative importance of each component in a sequence relative to the other components in that sequence. In natural language processing, importance is represented by \"soft\" weights assigned to each word in a sentence. More generally, attention encodes vectors called token embeddings across a fixed-width sequence that can range from tens to millions of tokens in size.\\nUnlike \"hard\" weights, which are computed during the backwards training pass, \"soft\" weights exist only in the forward pass and therefore change with every step of the input. Earlier designs implemented the attention mechanism in a serial recurrent neural network language translation system, but the later transformer design removed the slower sequential RNN and relied more heavily on the faster parallel attention scheme.\\nInspired by ideas about attention in humans, the attention mechanism was developed to address the weaknesses of leveraging information from the hidden layers of recurrent neural networks. Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence tends to be attenuated. Attention allows a token equal access to any part of a sentence directly, rather than only through the previous state.\\n\\n\\n== History ==\\n\\nAcademic reviews of the history of the attention mechanism are provided in Niu et al. and Soydaner.\\n\\n\\n=== Predecessors ===\\nSelective attention in humans had been well studied in neuroscience and cognitive psychology. In 1953, Colin Cherry studied selective attention in the context of audition, known as the cocktail party effect.\\nIn 1958, Donald Broadbent proposed the filter model of attention. Selective attention of vision was studied in the 1960s by George Sperling\\'s partial report paradigm. It was also noticed that saccade control is modulated by cognitive processes, insofar as the eye moves preferentially towards areas of high salience. As the fovea of the eye is small, the eye cannot sharply resolve the entire visual field at once. The use of saccade control allows the eye to quickly scan important features of a scene.\\nThese research developments inspired algorithms such as the Neocognitron and its variants. Meanwhile, developments in neural networks had inspired circuit models of biological visual attention. One well-cited network from 1998, for example, was inspired by the low-level primate visual system. It produced saliency maps of images using handcrafted (not learned) features, which were then used to guide a second neural network in processing patches of the image in order of reducing saliency.\\nA key aspect of attention mechanism can be written (schematically) as \\n  \\n    \\n      \\n        \\n          ∑\\n          \\n            i\\n          \\n        \\n        ⟨\\n        (\\n        \\n          query\\n        \\n        \\n          )\\n          \\n            i\\n          \\n        \\n        ,\\n        (\\n        \\n          key\\n        \\n        \\n          )\\n          \\n            i\\n          \\n        \\n        ⟩\\n        (\\n        \\n          value\\n        \\n        \\n          )\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\sum _{i}\\\\langle ({\\\\text{query}})_{i},({\\\\text{key}})_{i}\\\\rangle ({\\\\text{value}})_{i}}\\n  \\nwhere the angled brackets denote dot product. This shows that it involves a multiplicative operation. Multiplicative operations within artificial neural networks had been studied under the names of Group Method of Data Handling (1965) (where Kolmogorov-Gabor polynomials implement multiplicative units or \"gates\"), higher-order neural networks, multiplication units, sigma-pi units, fast weight controllers, and hyper-networks.\\n\\n\\n=== Linearized attention ===\\nJürgen Schmidhuber\\'s fast weight controller (1992) implements what was later called \"linearized attention\" or \"linear attention\" by Angelos Katharopoulos et al. (2020).  One of its two networks has  \"fast weights\" or \"dynamic links\" (1981). '), Document(metadata={'title': 'Adversarial machine learning', 'summary': 'Adversarial machine learning is the study of the attacks on machine learning algorithms, and of the defenses against such attacks. A survey from May 2020 exposes the fact that practitioners report a dire need for better protecting machine learning systems in industrial applications.\\nMost machine learning techniques are mostly designed to work on specific problem sets, under the assumption that the training and test data are generated from the same statistical distribution (IID). However, this assumption is often dangerously violated in practical high-stake applications, where users may intentionally supply fabricated data that violates the statistical assumption.\\nMost common attacks in adversarial machine learning include evasion attacks, data poisoning attacks, Byzantine attacks and model extraction.', 'source': 'https://en.wikipedia.org/wiki/Adversarial_machine_learning'}, page_content='Adversarial machine learning is the study of the attacks on machine learning algorithms, and of the defenses against such attacks. A survey from May 2020 exposes the fact that practitioners report a dire need for better protecting machine learning systems in industrial applications.\\nMost machine learning techniques are mostly designed to work on specific problem sets, under the assumption that the training and test data are generated from the same statistical distribution (IID). However, this assumption is often dangerously violated in practical high-stake applications, where users may intentionally supply fabricated data that violates the statistical assumption.\\nMost common attacks in adversarial machine learning include evasion attacks, data poisoning attacks, Byzantine attacks and model extraction.\\n\\n\\n== History ==\\nAt the MIT Spam Conference in January 2004, John Graham-Cumming showed that a machine-learning spam filter could be used to defeat another machine-learning spam filter by automatically learning which words to add to a spam email to get the email classified as not spam.\\nIn 2004, Nilesh Dalvi and others noted that linear classifiers used in spam filters could be defeated by simple \"evasion attacks\" as spammers inserted \"good words\" into their spam emails. (Around 2007, some spammers added random noise to fuzz words within \"image spam\" in order to defeat OCR-based filters.) In 2006, Marco Barreno and others published \"Can Machine Learning Be Secure?\", outlining a broad taxonomy of attacks. As late as 2013 many researchers continued to hope that non-linear classifiers (such as support vector machines and neural networks) might be robust to adversaries, until Battista Biggio and others demonstrated the first gradient-based attacks on such machine-learning models (2012–2013). In 2012, deep neural networks began to dominate computer vision problems; starting in 2014, Christian Szegedy and others demonstrated that deep neural networks could be fooled by adversaries, again using a gradient-based attack to craft adversarial perturbations.\\nRecently, it was observed that adversarial attacks are harder to produce in the practical world due to the different environmental constraints that cancel out the effect of noise. For example, any small rotation or slight illumination on an adversarial image can destroy the adversariality. In addition, researchers such as Google Brain\\'s Nicholas Frosst point out that it is much easier to make self-driving cars miss stop signs by physically removing the sign itself, rather than creating adversarial examples. Frosst also believes that the adversarial machine learning community incorrectly assumes models trained on a certain data distribution will also perform well on a completely different data distribution. He suggests that a new approach to machine learning should be explored, and is currently working on a unique neural network that has characteristics more similar to human perception than state-of-the-art approaches.\\nWhile adversarial machine learning continues to be heavily rooted in academia, large tech companies such as Google, Microsoft, and IBM have begun curating documentation and open source code bases to allow others to concretely assess the robustness of machine learning models and minimize the risk of adversarial attacks.\\n\\n\\n=== Examples ===\\nExamples include attacks in spam filtering, where spam messages are obfuscated through the misspelling of \"bad\" words or the insertion of \"good\" words; attacks in computer security, such as obfuscating malware code within network packets or modifying the characteristics of a network flow to mislead intrusion detection; attacks in biometric recognition where fake biometric traits may be exploited to impersonate a legitimate user; or to compromise users\\' template galleries that adapt to updated traits over time.\\nResearchers showed that by changing only one-pixel it was possible to fool deep learning algorithms. Others 3-D printed a toy turtle w'), Document(metadata={'title': 'Neural network (machine learning)', 'summary': 'In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a model inspired by the structure and function of biological neural networks in animal brains.\\nAn ANN consists of connected units or nodes called artificial neurons, which loosely model the neurons in the brain. These are connected by edges, which model the synapses in the brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The \"signal\" is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs, called the activation function. The strength of the signal at each connection is determined by a weight, which adjusts during the learning process.\\nTypically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least two hidden layers.\\nArtificial neural networks are used for various tasks, including predictive modeling, adaptive control, and solving problems in artificial intelligence. They can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Neural_network_(machine_learning)'}, page_content='In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a model inspired by the structure and function of biological neural networks in animal brains.\\nAn ANN consists of connected units or nodes called artificial neurons, which loosely model the neurons in the brain. These are connected by edges, which model the synapses in the brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The \"signal\" is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs, called the activation function. The strength of the signal at each connection is determined by a weight, which adjusts during the learning process.\\nTypically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least two hidden layers.\\nArtificial neural networks are used for various tasks, including predictive modeling, adaptive control, and solving problems in artificial intelligence. They can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information.\\n\\n\\n== Training ==\\nNeural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network\\'s parameters to minimize the difference, or empirical risk, between the predicted output and the actual target values in a given dataset. Gradient-based methods such as backpropagation are usually used to estimate the parameters of the network. During the training phase, ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. This method allows the network to generalize to unseen data.\\n\\n\\n== History ==\\n\\n\\n=== Early work ===\\nToday\\'s deep neural networks are based on early work in statistics over 200 years ago. The simplest kind of feedforward neural network (FNN) is a linear network, which consists of a single layer of output nodes with linear activation functions; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.\\nHistorically, digital computers such as the von Neumann model operate via the execution of explicit instructions with access to memory by a number of processors. Some neural networks, on the other hand, originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model, connectionist computing does not separate memory and processing.\\nWarren McCulloch and Walter Pitts (1943) considered a non-learning computational model for neural networks. This model paved the way for research to split into two approaches. One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence.\\nIn the late 1940s, D. O. Hebb proposed a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. It was used in many early neural networks, such as Rosenblatt\\'s perception and the Hopfield network. Farley and Clark (1954) used computational machines to simulate a Hebbian network. Other neural network computational machines were created by Rochester,'), Document(metadata={'title': 'Boosting (machine learning)', 'summary': 'In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, variance. It is used in supervised learning and a family of machine learning algorithms that convert weak learners to strong ones.\\nThe concept of boosting is based on the question posed by Kearns and Valiant (1988, 1989): \"Can a set of weak learners create a single strong learner?\" A weak learner is defined as a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). A strong learner is a classifier that is arbitrarily well-correlated with the true classification. Robert Schapire answered the question in the affirmative in a paper published in 1990.This has had significant ramifications in machine learning and statistics, most notably leading to the development of boosting.\\nInitially, the hypothesis boosting problem simply referred to the process of turning a weak learner into a strong learner. Algorithms that achieve this quickly became known as \"boosting\". Freund and Schapire\\'s arcing (Adapt[at]ive Resampling and Combining), as a general technique, is more or less synonymous with boosting.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Boosting_(machine_learning)'}, page_content='In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, variance. It is used in supervised learning and a family of machine learning algorithms that convert weak learners to strong ones.\\nThe concept of boosting is based on the question posed by Kearns and Valiant (1988, 1989): \"Can a set of weak learners create a single strong learner?\" A weak learner is defined as a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). A strong learner is a classifier that is arbitrarily well-correlated with the true classification. Robert Schapire answered the question in the affirmative in a paper published in 1990.This has had significant ramifications in machine learning and statistics, most notably leading to the development of boosting.\\nInitially, the hypothesis boosting problem simply referred to the process of turning a weak learner into a strong learner. Algorithms that achieve this quickly became known as \"boosting\". Freund and Schapire\\'s arcing (Adapt[at]ive Resampling and Combining), as a general technique, is more or less synonymous with boosting.\\n\\n\\n== Algorithms ==\\nWhile boosting is not algorithmically constrained, most boosting algorithms consist of iteratively learning weak classifiers with respect to a distribution and adding them to a final strong classifier. When they are added, they are weighted in a way that is related to the weak learners\\' accuracy.  After a weak learner is added, the data weights are readjusted, known as \"re-weighting\". Misclassified input data gain a higher weight and examples that are classified correctly lose weight. Thus, future weak learners focus more on the examples that previous weak learners misclassified.\\n\\nThere are many boosting algorithms. The original ones, proposed by Robert Schapire (a recursive majority gate formulation), and Yoav Freund (boost by majority), were not adaptive and could not take full advantage of the weak learners. Schapire and Freund then developed AdaBoost, an adaptive boosting algorithm that won the prestigious Gödel Prize.\\nOnly algorithms that are provable boosting algorithms in the probably approximately correct learning formulation can accurately be called boosting algorithms.  Other algorithms that are similar in spirit to boosting algorithms are sometimes called \"leveraging algorithms\", although they are also sometimes incorrectly called boosting algorithms.\\nThe main variation between many boosting algorithms is their method of weighting training data points and hypotheses. AdaBoost is very popular and the most significant historically as it was the first algorithm that could adapt to the weak learners. It is often the basis of introductory coverage of boosting in university machine learning courses. There are many more recent algorithms such as LPBoost, TotalBoost, BrownBoost, xgboost, MadaBoost, LogitBoost, and others. Many boosting algorithms fit into the AnyBoost framework, which shows that boosting performs gradient descent in a function space using a convex cost function.\\n\\n\\n== Object categorization in computer vision ==\\n\\nGiven images containing various known objects in the world, a classifier can be learned from them to automatically classify the objects in future images.  Simple classifiers built based on some image feature of the object tend to be weak in categorization performance. Using boosting methods for object categorization is a way to unify the weak classifiers in a special way to boost the overall ability of categorization.\\n\\n\\n=== Problem of object categorization ===\\nObject categorization is a typical task of computer vision that involves determining whether or not an image contains some specific category of object.  The idea is closely related with recognition, identification, and detection.  Appearance based object categorization typically contains feature extraction, learning a classifier, and applying the classifier to new examples.  There a'), Document(metadata={'title': 'Timeline of machine learning', 'summary': 'This page is a timeline of machine learning. Major discoveries, achievements, milestones and other major events in machine learning are included.', 'source': 'https://en.wikipedia.org/wiki/Timeline_of_machine_learning'}, page_content='This page is a timeline of machine learning. Major discoveries, achievements, milestones and other major events in machine learning are included.\\n\\n\\n== Overview ==\\n\\n\\n== Timeline ==\\n\\n\\n== See also ==\\nHistory of artificial intelligence\\nTimeline of artificial intelligence\\nTimeline of machine translation\\n\\n\\n== References ==\\n\\n\\n=== Citations ===\\n\\n\\n=== Works cited ===\\nCrevier, Daniel (1993). AI: The Tumultuous Search for Artificial Intelligence. New York: BasicBooks. ISBN 0-465-02997-3.\\nMarr, Bernard (19 February 2016). \"A Short History of Machine Learning -- Every Manager Should Read\". Forbes. Archived from the original on 2022-12-05. Retrieved 2022-12-25.\\nRussell, Stuart; Norvig, Peter (2003). Artificial Intelligence: A Modern Approach. London: Pearson Education. ISBN 0-137-90395-2.'), Document(metadata={'title': 'Active learning (machine learning)', 'summary': 'Active learning is a special case of machine learning in which a learning algorithm can interactively query a human user (or some other information source), to label new data points with the desired outputs. The human user must possess knowledge/expertise in the problem domain, including the ability to consult/research authoritative sources when necessary.  In statistics literature, it is sometimes also called optimal experimental design. The information source is also called teacher or oracle.\\nThere are situations in which unlabeled data is abundant but manual labeling is expensive. In such a scenario, learning algorithms can actively query the user/teacher for labels. This type of iterative supervised learning is called active learning. Since the learner chooses the examples, the number of examples to learn a concept can often be much lower than the number required in normal supervised learning. With this approach, there is a risk that the algorithm is overwhelmed by uninformative examples.  Recent developments are dedicated to multi-label active learning, hybrid active learning and active learning in a single-pass (on-line) context, combining concepts from the field of machine learning (e.g. conflict and ignorance) with adaptive, incremental learning policies in the field of online machine learning. Using active learning allows for faster development of a machine learning algorithm, when comparative updates would require a quantum or super computer.\\nLarge-scale active learning projects may benefit from crowdsourcing frameworks such as Amazon Mechanical Turk that include many humans in the active learning loop.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Active_learning_(machine_learning)'}, page_content='Active learning is a special case of machine learning in which a learning algorithm can interactively query a human user (or some other information source), to label new data points with the desired outputs. The human user must possess knowledge/expertise in the problem domain, including the ability to consult/research authoritative sources when necessary.  In statistics literature, it is sometimes also called optimal experimental design. The information source is also called teacher or oracle.\\nThere are situations in which unlabeled data is abundant but manual labeling is expensive. In such a scenario, learning algorithms can actively query the user/teacher for labels. This type of iterative supervised learning is called active learning. Since the learner chooses the examples, the number of examples to learn a concept can often be much lower than the number required in normal supervised learning. With this approach, there is a risk that the algorithm is overwhelmed by uninformative examples.  Recent developments are dedicated to multi-label active learning, hybrid active learning and active learning in a single-pass (on-line) context, combining concepts from the field of machine learning (e.g. conflict and ignorance) with adaptive, incremental learning policies in the field of online machine learning. Using active learning allows for faster development of a machine learning algorithm, when comparative updates would require a quantum or super computer.\\nLarge-scale active learning projects may benefit from crowdsourcing frameworks such as Amazon Mechanical Turk that include many humans in the active learning loop.\\n\\n\\n== Definitions ==\\nLet T be the total set of all data under consideration. For example, in a protein engineering problem, T would include all proteins that are known to have a certain interesting activity and all additional proteins that one might want to test for that activity.\\nDuring each iteration, i, T is broken up into three subsets\\n\\n  \\n    \\n      \\n        \\n          \\n            T\\n          \\n          \\n            K\\n            ,\\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbf {T} _{K,i}}\\n  \\n: Data points where the label is known.\\n\\n  \\n    \\n      \\n        \\n          \\n            T\\n          \\n          \\n            U\\n            ,\\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbf {T} _{U,i}}\\n  \\n: Data points where the label is unknown.\\n\\n  \\n    \\n      \\n        \\n          \\n            T\\n          \\n          \\n            C\\n            ,\\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbf {T} _{C,i}}\\n  \\n: A subset of TU,i that is chosen to be labeled.\\nMost of the current research in active learning involves the best method to choose the data points for TC,i.\\n\\n\\n== Scenarios ==\\nPool-Based Sampling: In this approach, which is the most well known scenario, the learning algorithm attempts to evaluate the entire dataset before selecting data points (instances) for labeling. It is often initially trained on a fully labeled subset of the data using a machine-learning method such as logistic regression or SVM that yields class-membership probabilities for individual data instances. The candidate instances are those for which the prediction is most ambiguous. Instances are drawn from the entire data pool and assigned a confidence score, a measurement of how well the learner \"understands\" the data. The system then selects the instances for which it is the least confident and queries the teacher for the labels. The theoretical drawback of pool-based sampling is that it is memory-intensive and is therefore limited in its capacity to handle enormous datasets, but in practice, the rate-limiting factor is that the teacher is typically a (fatiguable) human expert who must be paid for their effort, rather than computer memory.\\nStream-Based Selective Sampling: Here, each consecutive unlabeled instance is examined one at a time with the machine evaluating the informati'), Document(metadata={'title': 'Quantum machine learning', 'summary': 'Quantum machine learning is the integration of quantum algorithms within machine learning programs.\\nThe most common use of the term refers to machine learning algorithms for the analysis of classical data executed on a quantum computer, i.e. quantum-enhanced machine learning. While machine learning algorithms are used to compute immense quantities of data, quantum machine learning utilizes qubits and quantum operations or specialized quantum systems to improve computational speed and data storage done by algorithms in a program. This includes hybrid methods that involve both classical and quantum processing, where computationally difficult subroutines are outsourced to a quantum device. These routines can be more complex in nature and executed faster on a quantum computer. Furthermore, quantum algorithms can be used to analyze quantum states instead of classical data.\\nBeyond quantum computing, the term \"quantum machine learning\" is also associated with classical machine learning methods applied to data generated from quantum experiments (i.e. machine learning of quantum systems), such as learning the phase transitions of a quantum system or creating new quantum experiments.\\nQuantum machine learning also extends to a branch of research that explores methodological and structural similarities between certain physical systems and learning systems, in particular neural networks. For example, some mathematical and numerical techniques from quantum physics are applicable to classical deep learning and vice versa.\\nFurthermore, researchers investigate more abstract notions of learning theory with respect to quantum information, sometimes referred to as \"quantum learning theory\".\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Quantum_machine_learning'}, page_content='Quantum machine learning is the integration of quantum algorithms within machine learning programs.\\nThe most common use of the term refers to machine learning algorithms for the analysis of classical data executed on a quantum computer, i.e. quantum-enhanced machine learning. While machine learning algorithms are used to compute immense quantities of data, quantum machine learning utilizes qubits and quantum operations or specialized quantum systems to improve computational speed and data storage done by algorithms in a program. This includes hybrid methods that involve both classical and quantum processing, where computationally difficult subroutines are outsourced to a quantum device. These routines can be more complex in nature and executed faster on a quantum computer. Furthermore, quantum algorithms can be used to analyze quantum states instead of classical data.\\nBeyond quantum computing, the term \"quantum machine learning\" is also associated with classical machine learning methods applied to data generated from quantum experiments (i.e. machine learning of quantum systems), such as learning the phase transitions of a quantum system or creating new quantum experiments.\\nQuantum machine learning also extends to a branch of research that explores methodological and structural similarities between certain physical systems and learning systems, in particular neural networks. For example, some mathematical and numerical techniques from quantum physics are applicable to classical deep learning and vice versa.\\nFurthermore, researchers investigate more abstract notions of learning theory with respect to quantum information, sometimes referred to as \"quantum learning theory\".\\n\\n\\n== Machine learning with quantum computers ==\\nQuantum-enhanced machine learning refers to quantum algorithms that solve tasks in machine learning, thereby improving and often expediting classical machine learning techniques. Such algorithms typically require one to encode the given classical data set into a quantum computer to make it accessible for quantum information processing. Subsequently, quantum information processing routines are applied and the result of the quantum computation is read out by measuring the quantum system. For example, the outcome of the measurement of a qubit reveals the result of a binary classification task. While many proposals of quantum machine learning algorithms are still purely theoretical and require a full-scale universal quantum computer to be tested, others have been implemented on small-scale or special purpose quantum devices.\\n\\n\\n=== Quantum associative memories and quantum pattern recognition ===\\nAssociative (or content-addressable memories) are able to recognize stored content on the basis of a similarity measure, rather than fixed addresses, like in random access memories. As such they must be able to retrieve both incomplete and corrupted patterns, the essential machine learning task of pattern recognition.\\nTypical classical associative memories store p patterns in the \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n  \\n interactions (synapses) of a real,  symmetric energy matrix over a network of n artificial neurons. The encoding is such that the desired patterns are local minima of the energy functional and retrieval is done by minimizing the total energy, starting from an initial configuration.\\nUnfortunately, classical associative memories are severely limited by the phenomenon of cross-talk. When too many patterns are stored, spurious memories appear which quickly proliferate, so that the energy landscape becomes disordered and no retrieval is anymore possible. The number of storable patterns is typically limited by a linear function of the number of neurons, \\n  \\n    \\n      \\n        p\\n        ≤\\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle p\\\\leq O(n)}\\n  \\n.\\nQuantum associative memories'), Document(metadata={'title': 'Ensemble learning', 'summary': 'In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.\\nUnlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Ensemble_learning'}, page_content='In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.\\nUnlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.\\n\\n\\n== Overview ==\\nSupervised learning algorithms perform the task of searching through a hypothesis space to find a suitable hypothesis that will make good predictions with a particular problem. Even if the hypothesis space contains hypotheses that are very well-suited for a particular problem, it may be very difficult to find a good one. Ensembles combine multiple hypotheses to form a (hopefully) better hypothesis.\\nEnsemble learning trains two or more Machine Learning algorithms to a specific classification or regression task. The algorithms within the ensemble learning model are generally referred as \"base models\", \"base learners\" or \"weak learners\" in literature. The base models can be constructed using a single modelling algorithm or several different algorithms. The idea is train a diverse collection of weak performing models to the same modelling task. As a result, the predicted or classified outcomes of each weak learner have poor predictive ability (high bias, i.e. high model errors) and among the collection of all weak learners the outcome and error values exhibit high variance. Fundamentally, an ensemble learning model trains many (at least 2) high-bias (weak) and high-variance (diverse) models to be combined into a stronger and better performing model. Essentially, it\\'s a set of algorithmic models — which would not produce satisfactory predictive results individually — that gets combined or averaged over all base models to produce a single high performing, accurate and low-variance model to fit the task as required.\\nEnsemble learning typically refers to Bagging (bootstrap-aggregating), Boosting or Stacking/Blending techniques to induce high variability among the base models. Bagging creates diversity by generating random samples from the training observations and fitting the same model to each different sample — also known as \"homogeneous parallel ensembles\". Boosting follows an iterative process by sequentially training each next base model on the up-weighted errors of the previous base model\\'s errors, producing an additive model to reduce the final model errors — also known as \"sequential ensemble learning\". Stacking or Blending consists of different base models, each trained independently (i.e. diverse/high variability) to be combined into the ensemble model — producing a \"heterogeneous parallel ensemble\". Common applications of ensemble learning include Random Forests (extension of Baggin), Boosted Tree-Models, Gradient Boosted Tree-Models and models in applications of stacking are generally more task-specific — such as combing clustering techniques with other parametric and/or non-parametric techniques. (See here for a comprehensive overview:\\nThe broader term of multiple classifier systems also covers hybridization of hypotheses that are not induced by the same base learner.\\nEvaluating the prediction of an ensemble typically requires more computation than evaluating the prediction of a single model. In one sense, ensemble learning may be thought of as a way to compensate for poor learning algorithms by performing a lot of extra computation. On the other hand, the alternative is to do a lot more learning on one non-ensemble system. An ensemble system may be more efficient at improving overall accuracy for the same increase in compute, storage, or communication resources by using that increase on two or more methods, than would have been improved by increasing resource use for a single method.  Fast algorithms such as decision trees are commonly used in ens'), Document(metadata={'title': 'Transformer (deep learning architecture)', 'summary': 'A transformer is a deep learning architecture developed by researchers at Google and based on the multi-head attention mechanism, proposed in a 2017 paper \"Attention Is All You Need\". Text is converted to numerical representations called tokens, and each token is converted into a vector via looking up from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism allowing the signal for key tokens to be amplified and less important tokens to be diminished.\\nTransformers have the advantage of having no recurrent units, and therefore require less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLM) on large (language) datasets, such as the Wikipedia corpus and Common Crawl.\\n\\nTransformers were first developed as an improvement over previous architectures for machine translation, but have found many applications since then. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning, audio, multi-modal processing, robotics, and even playing chess. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers).', 'source': 'https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)'}, page_content='A transformer is a deep learning architecture developed by researchers at Google and based on the multi-head attention mechanism, proposed in a 2017 paper \"Attention Is All You Need\". Text is converted to numerical representations called tokens, and each token is converted into a vector via looking up from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism allowing the signal for key tokens to be amplified and less important tokens to be diminished.\\nTransformers have the advantage of having no recurrent units, and therefore require less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLM) on large (language) datasets, such as the Wikipedia corpus and Common Crawl.\\n\\nTransformers were first developed as an improvement over previous architectures for machine translation, but have found many applications since then. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning, audio, multi-modal processing, robotics, and even playing chess. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers).\\n\\n\\n== History ==\\n\\n\\n=== Predecessors ===\\nFor many years, sequence modelling and generation was done by using plain recurrent neural networks (RNNs). A well-cited early example was the Elman network (1990). In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the model\\'s state at the end of a long sentence without precise, extractable information about preceding tokens.\\nA key breakthrough was LSTM (1995), a RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling. One key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons, so-called multiplicative units. Neural networks using multiplicative units were later called sigma-pi networks or higher-order networks. LSTM became the standard architecture for long sequence modelling until the 2017 publication of Transformers.\\nHowever, LSTM still used sequential processing, like most other RNNs. Specifically, RNNs operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence. \\nModern Transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window.  The linearly scaling fast weight controller (1992) learns  to compute a  weight matrix for further processing depending on the input. One of its two networks has  \"fast weights\" or \"dynamic links\" (1981). A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries. This was later shown to be equivalent to the unnormalized linear Transformer.\\n\\n\\n=== Attention with seq2seq ===\\n\\nThe idea of encoder-decoder sequence transduction had been developed in the early 2010s (see  for previous papers). The papers most commonly cited as the originators that produced seq2seq are two concurrently published papers from 2014.\\n(Sutskever et al, 2014)  was a 380M-parameter model for machine translation using two long short-term memory (LSTM). The architecture consists of two parts. The encoder is an LSTM that takes in a sequence of tokens and turns it into a vector. The decoder is another LSTM that converts the vector into a sequence of tokens. Similarly, (Cho et al, 2014) was 130M-parameter model that used gated recurrent units (GRU) instead of LSTM. Later research showed that GRUs are n'), Document(metadata={'title': 'Feature (machine learning)', 'summary': 'In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon. Choosing informative, discriminating and independent features is a crucial element of effective algorithms in pattern recognition,  classification and regression. Features are usually numeric, but structural features such as strings and graphs are used in syntactic pattern recognition. The concept of \"feature\" is related to that of explanatory variable used in statistical techniques such as linear regression.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Feature_(machine_learning)'}, page_content='In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon. Choosing informative, discriminating and independent features is a crucial element of effective algorithms in pattern recognition,  classification and regression. Features are usually numeric, but structural features such as strings and graphs are used in syntactic pattern recognition. The concept of \"feature\" is related to that of explanatory variable used in statistical techniques such as linear regression.\\n\\n\\n== Feature types ==\\nIn feature engineering, two types of features are commonly used: numerical and categorical.\\nNumerical features are continuous values that can be measured on a scale. Examples of numerical features include age, height, weight, and income. Numerical features can be used in machine learning algorithms directly.\\nCategorical features are discrete values that can be grouped into categories. Examples of categorical features include gender, color, and zip code. Categorical features typically need to be converted to numerical features before they can be used in machine learning algorithms. This can be done using a variety of techniques, such as one-hot encoding, label encoding, and ordinal encoding.\\nThe type of feature that is used in feature engineering depends on the specific machine learning algorithm that is being used. Some machine learning algorithms, such as decision trees, can handle both numerical and categorical features. Other machine learning algorithms, such as linear regression, can only handle numerical features.\\n\\n\\n== Classification ==\\nA numeric feature can be conveniently described by a feature vector. One way to achieve binary classification is using a linear predictor function (related to the perceptron) with a feature vector as input. The method consists of calculating the scalar product between the feature vector and a vector of weights, qualifying those observations whose result exceeds a threshold.\\nAlgorithms for classification from a feature vector include nearest neighbor classification, neural networks, and statistical techniques such as Bayesian approaches.\\n\\n\\n== Examples ==\\n\\nIn character recognition, features may include histograms counting the number of black pixels along horizontal and vertical directions, number of internal holes, stroke detection and many others.\\nIn speech recognition, features for recognizing phonemes can include noise ratios, length of sounds, relative power, filter matches and many others.\\nIn spam detection algorithms, features may include the presence or absence of certain email headers, \\nthe email structure, the language, the frequency of specific terms, the grammatical correctness of the text.\\nIn computer vision, there are a large number of possible features, such as edges and objects.\\n\\n\\n== Feature vectors ==\\n\\nIn pattern recognition and machine learning, a feature vector is an n-dimensional vector of numerical features that represent some object. Many algorithms in machine learning require a numerical representation of objects, since such representations facilitate processing and statistical analysis. When representing images, the feature values might correspond to the pixels of an image, while when representing texts the features might be the frequencies of occurrence of textual terms. Feature vectors are equivalent to the vectors of explanatory variables used in statistical procedures such as linear regression.  Feature vectors are often combined with weights using a dot product in order to construct a linear predictor function that is used to determine a score for making a prediction.\\nThe vector space associated with these vectors is often called the feature space. In order to reduce the dimensionality of the feature space, a number of dimensionality reduction techniques can be employed.\\nHigher-level features can be obtained from already available features and added to the feature vector; for example, for the study of diseases th'), Document(metadata={'title': 'Artificial intelligence', 'summary': 'Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.\\nSome high-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); interacting via human speech (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT, and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it\\'s not labeled AI anymore.\"\\nThe various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence—the ability to complete any task performable by a human on an at least equal level—is among the field\\'s long-term goals. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.\\nArtificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism, followed by periods of disappointment and loss of funding, known as AI winter. Funding and interest vastly increased after 2012 when deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture, and by the early 2020s hundreds of billions of dollars were being invested in AI (known as the \"AI boom\"). The widespread use of AI in the 21st century exposed several unintended consequences and harms in the present and raised concerns about its risks and long-term effects in the future, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Artificial_intelligence'}, page_content='Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.\\nSome high-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); interacting via human speech (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT, and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it\\'s not labeled AI anymore.\"\\nThe various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence—the ability to complete any task performable by a human on an at least equal level—is among the field\\'s long-term goals. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.\\nArtificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism, followed by periods of disappointment and loss of funding, known as AI winter. Funding and interest vastly increased after 2012 when deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture, and by the early 2020s hundreds of billions of dollars were being invested in AI (known as the \"AI boom\"). The widespread use of AI in the 21st century exposed several unintended consequences and harms in the present and raised concerns about its risks and long-term effects in the future, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\\n\\n\\n== Goals ==\\nThe general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.\\n\\n\\n=== Reasoning and problem-solving ===\\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.\\nMany of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem.\\n\\n\\n=== Knowledge representation ===\\n\\nKnowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and a'), Document(metadata={'title': 'Hyperparameter (machine learning)', 'summary': \"In machine learning, a hyperparameter is a parameter that can be set in order to define any configurable part of a model's learning process. Hyperparameters can be classified as either model hyperparameters (such as the topology and size of a neural network) or algorithm hyperparameters (such as the learning rate and the batch size of an optimizer). These are named hyperparameters in contrast to parameters, which are characteristics that the model learns from the data.\\nHyperparameters are not required by every model or algorithm. Some simple algorithms such as ordinary least squares regression require none. However, the LASSO algorithm, for example, adds a regularization hyperparameter to ordinary least squares which must be set before training. Even models and algorithms without a strict requirement to define hyperparameters may not produce meaningful results if these are not carefully chosen. However, optimal values for hyperparameters are not always easy to predict. Some hyperparameters may have no meaningful effect, or one important variable may be conditional upon the value of another. Often a separate process of hyperparameter tuning is needed to find a suitable combination for the data and task. \\nAs well was improving model performance, hyperparameters can be used to by researchers introduce robustness and reproducibility into their work, especially if it uses models that incorporate random number generation.\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)'}, page_content=\"In machine learning, a hyperparameter is a parameter that can be set in order to define any configurable part of a model's learning process. Hyperparameters can be classified as either model hyperparameters (such as the topology and size of a neural network) or algorithm hyperparameters (such as the learning rate and the batch size of an optimizer). These are named hyperparameters in contrast to parameters, which are characteristics that the model learns from the data.\\nHyperparameters are not required by every model or algorithm. Some simple algorithms such as ordinary least squares regression require none. However, the LASSO algorithm, for example, adds a regularization hyperparameter to ordinary least squares which must be set before training. Even models and algorithms without a strict requirement to define hyperparameters may not produce meaningful results if these are not carefully chosen. However, optimal values for hyperparameters are not always easy to predict. Some hyperparameters may have no meaningful effect, or one important variable may be conditional upon the value of another. Often a separate process of hyperparameter tuning is needed to find a suitable combination for the data and task. \\nAs well was improving model performance, hyperparameters can be used to by researchers introduce robustness and reproducibility into their work, especially if it uses models that incorporate random number generation.\\n\\n\\n== Considerations ==\\nThe time required to train and test a model can depend upon the choice of its hyperparameters. A hyperparameter is usually of continuous or integer type, leading to mixed-type optimization problems. The existence of some hyperparameters is conditional upon the value of others, e.g. the size of each hidden layer in a neural network can be conditional upon the number of layers.\\n\\n\\n=== Difficulty-learnable parameters ===\\nThe objective function is typically non-differentiable with respect to hyperparameters. As a result, in most instances, hyperparameters cannot be learned using gradient-based optimization methods (such as gradient descent), which are commonly employed to learn model parameters. These hyperparameters are those parameters describing a model representation that cannot be learned by common optimization methods, but nonetheless affect the loss function. An example would be the tolerance hyperparameter for errors in support vector machines.\\n\\n\\n=== Untrainable parameters ===\\nSometimes, hyperparameters cannot be learned from the training data because they aggressively increase the capacity of a model and can push the loss function to an undesired minimum (overfitting to the data), as opposed to correctly mapping the richness of the structure in the data. For example, if we treat the degree of a polynomial equation fitting a regression model as a trainable parameter, the degree would increase until the model perfectly fit the data, yielding low training error, but poor generalization performance.\\n\\n\\n=== Tunability ===\\nMost performance variation can be attributed to just a few hyperparameters. The tunability of an algorithm, hyperparameter, or interacting hyperparameters is a measure of how much performance can be gained by tuning it. For an LSTM, while the learning rate followed by the network size are its most crucial hyperparameters, batching and momentum have no significant effect on its performance.\\nAlthough some research has advocated the use of mini-batch sizes in the thousands, other work has found the best performance with mini-batch sizes between 2 and 32.\\n\\n\\n=== Robustness ===\\nAn inherent stochasticity in learning directly implies that the empirical hyperparameter performance is not necessarily its true performance. Methods that are not robust to simple changes in hyperparameters, random seeds, or even different implementations of the same algorithm cannot be integrated into mission critical control systems without significant simplification and robustification.\\nReinforcement learn\"), Document(metadata={'title': 'Support vector machine', 'summary': 'In machine learning, support vector machines (SVMs, also support vector networks) are supervised max-margin models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995, Vapnik et al., 1997) SVMs are one of the most studied models, being based on statistical learning frameworks of VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). \\nIn addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, representing the data only through a set of pairwise similarity comparisons between the original data points using a kernel function, which transforms them into coordinates in the higher dimensional feature space. Thus, SVMs use the kernel trick to implicitly map their inputs into high-dimensional feature spaces where linear classification can be performed.  Being max-margin models, SVMs are resilient to noisy data (for example, mis-classified examples). SVMs can also be used for regression tasks, where the objective becomes \\n  \\n    \\n      \\n        ϵ\\n      \\n    \\n    {\\\\displaystyle \\\\epsilon }\\n  \\n-sensitive.\\nThe support vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data. These data sets require unsupervised learning approaches, which attempt to find natural clustering of the data to groups and, then, to map new data according to these clusters. \\nThe popularity of SVMs is likely due to their amenability to theoretical analysis, their flexibility in being applied to a wide variety of tasks, including structured prediction problems. It is not clear that SVMs have better predictive performance than other linear models, such as logistic regression and linear regression.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Support_vector_machine'}, page_content='In machine learning, support vector machines (SVMs, also support vector networks) are supervised max-margin models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995, Vapnik et al., 1997) SVMs are one of the most studied models, being based on statistical learning frameworks of VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). \\nIn addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, representing the data only through a set of pairwise similarity comparisons between the original data points using a kernel function, which transforms them into coordinates in the higher dimensional feature space. Thus, SVMs use the kernel trick to implicitly map their inputs into high-dimensional feature spaces where linear classification can be performed.  Being max-margin models, SVMs are resilient to noisy data (for example, mis-classified examples). SVMs can also be used for regression tasks, where the objective becomes \\n  \\n    \\n      \\n        ϵ\\n      \\n    \\n    {\\\\displaystyle \\\\epsilon }\\n  \\n-sensitive.\\nThe support vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data. These data sets require unsupervised learning approaches, which attempt to find natural clustering of the data to groups and, then, to map new data according to these clusters. \\nThe popularity of SVMs is likely due to their amenability to theoretical analysis, their flexibility in being applied to a wide variety of tasks, including structured prediction problems. It is not clear that SVMs have better predictive performance than other linear models, such as logistic regression and linear regression.\\n\\n\\n== Motivation ==\\n\\nClassifying data is a common task in machine learning.\\nSuppose some given data points each belong to one of two classes, and the goal is to decide which class a new data point will be in. In the case of support vector machines, a data point is viewed as a \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n  \\n-dimensional vector (a list of \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n  \\n numbers), and we want to know whether we can separate such points with a \\n  \\n    \\n      \\n        (\\n        p\\n        −\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle (p-1)}\\n  \\n-dimensional hyperplane. This is called a linear classifier. There are many hyperplanes that might classify the data. One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two classes. So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it defines is known as a maximum-margin classifier; or equivalently, the perceptron of optimal stability.\\nMore formally, a support vector machine constructs a hyperplane or set of hyperplanes in a high or infinite-dimensional space, which can be used for classification, regression, or other tasks like outliers detection. Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class (so-called functional margin), since in general the larger the margin, the lower the generalization error of the classifier. A lower generalization error means that the implementer is less likely to experience overfitting.\\n\\nWhereas the original problem may be stated in a finite-dimensional space, it often happens that the sets to discriminate are not linearly separable in that space. For this reason, it was proposed that the original finite-dimensional '), Document(metadata={'title': 'Machine Learning (journal)', 'summary': 'Machine Learning  is a peer-reviewed scientific journal, published since 1986.\\nIn 2001, forty editors and members of the editorial board of Machine Learning resigned in order to support the Journal of Machine Learning Research (JMLR), saying that in the era of the internet, it was detrimental for researchers to continue publishing their papers in expensive journals with pay-access archives. Instead, they wrote, they supported the model of JMLR, in which authors retained copyright over their papers and archives were freely available on the internet.\\nFollowing the mass resignation, Kluwer changed their publishing policy to allow authors to self-archive their papers online after peer-review.', 'source': 'https://en.wikipedia.org/wiki/Machine_Learning_(journal)'}, page_content='Machine Learning  is a peer-reviewed scientific journal, published since 1986.\\nIn 2001, forty editors and members of the editorial board of Machine Learning resigned in order to support the Journal of Machine Learning Research (JMLR), saying that in the era of the internet, it was detrimental for researchers to continue publishing their papers in expensive journals with pay-access archives. Instead, they wrote, they supported the model of JMLR, in which authors retained copyright over their papers and archives were freely available on the internet.\\nFollowing the mass resignation, Kluwer changed their publishing policy to allow authors to self-archive their papers online after peer-review.\\n\\n\\n== Selected articles ==\\nJ.R. Quinlan (1986). \"Induction of Decision Trees\". Machine Learning. 1: 81–106. doi:10.1007/BF00116251.\\nNick Littlestone (1988). \"Learning Quickly When Irrelevant Attributes Abound: A New Linear-threshold Algorithm\" (PDF). Machine Learning. 2 (4): 285–318. doi:10.1007/BF00116827.\\n\\nJohn R. Anderson and Michael Matessa (1992). \"Explorations of an Incremental, Bayesian Algorithm for Categorization\". Machine Learning. 9 (4): 275–308. doi:10.1007/BF00994109.\\nDavid Klahr (1994). \"Children, Adults, and Machines as Discovery Systems\". Machine Learning. 14 (3): 313–320. doi:10.1007/BF00993981.\\nThomas Dean and Dana Angluin and Kenneth Basye and Sean Engelson and Leslie Kaelbling and Evangelos Kokkevis and Oded Maron (1995). \"Inferring Finite Automata with Stochastic Output Functions and an Application to Map Learning\". Machine Learning. 18: 81–108. doi:10.1007/BF00993822.\\nLuc De Raedt and Luc Dehaspe (1997). \"Clausal Discovery\". Machine Learning. 26 (2/3): 99–146. doi:10.1023/A:1007361123060.\\nC. de la Higuera (1997). \"Characteristic Sets for Grammatical Inference\". Machine Learning. 27: 1–14.\\nRobert E. Schapire and Yoram Singer (1999). \"Improved Boosting Algorithms Using Confidence-rated Predictions\". Machine Learning. 37 (3): 297–336. doi:10.1023/A:1007614523901.\\nRobert E. Schapire and Yoram Singer (2000). \"BoosTexter: A Boosting-based System for Text Categorization\". Machine Learning. 39 (2/3): 135–168. doi:10.1023/A:1007649029923.\\nP. Rossmanith and T. Zeugmann (2001). \"Stochastic Finite Learning of the Pattern Languages\". Machine Learning. 44 (1–2): 67–91. doi:10.1023/A:1010875913047.\\nParekh, Rajesh; Honavar, Vasant (2001). \"Learning DFA from Simple Examples\". Machine Learning. 44 (1/2): 9–35. doi:10.1023/A:1010822518073.\\nAyhan Demiriz and Kristin P. Bennett and John Shawe-Taylor (2002). \"Linear Programming Boosting via Column Generation\". Machine Learning. 46: 225–254. doi:10.1023/A:1012470815092.\\nSimon Colton and Stephen Muggleton (2006). \"Mathematical Applications of Inductive Logic Programming\" (PDF). Machine Learning. 64 (1–3): 25–64. doi:10.1007/s10994-006-8259-x.\\nWill Bridewell and Pat Langley and Ljupco Todorovski and Saso Dzeroski (2008). \"Inductive Process Modeling\". Machine Learning.\\nStephen Muggleton and Alireza Tamaddoni-Nezhad (2008). \"QG/GA: a stochastic search for Progol\". Machine Learning. 70 (2–3): 121–133. doi:10.1007/s10994-007-5029-3.\\n\\n\\n== References =='), Document(metadata={'title': 'Automated machine learning', 'summary': 'Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. It is the combination of automation and ML. \\nAutoML potentially includes every stage from beginning with a raw dataset to building a machine learning model ready for deployment. AutoML was proposed as an artificial intelligence-based solution to the growing challenge of applying machine learning. The high degree of automation in AutoML aims to allow non-experts to make use of machine learning models and techniques without requiring them to become experts in machine learning. Automating the process of applying machine learning end-to-end additionally offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform hand-designed models. \\nCommon techniques used in AutoML include hyperparameter optimization, meta-learning and neural architecture search.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Automated_machine_learning'}, page_content='Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. It is the combination of automation and ML. \\nAutoML potentially includes every stage from beginning with a raw dataset to building a machine learning model ready for deployment. AutoML was proposed as an artificial intelligence-based solution to the growing challenge of applying machine learning. The high degree of automation in AutoML aims to allow non-experts to make use of machine learning models and techniques without requiring them to become experts in machine learning. Automating the process of applying machine learning end-to-end additionally offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform hand-designed models. \\nCommon techniques used in AutoML include hyperparameter optimization, meta-learning and neural architecture search.\\n\\n\\n== Comparison to the standard approach ==\\nIn a typical machine learning application, practitioners have a set of input data points to be used for training. The raw data may not be in a form that all algorithms can be applied to. To make the data amenable for machine learning, an expert may have to apply appropriate data pre-processing, feature engineering, feature extraction, and feature selection methods. After these steps, practitioners must then perform algorithm selection and hyperparameter optimization to maximize the predictive performance of their model. If deep learning is used, the architecture of the neural network must also be chosen manually by the machine learning expert. \\nEach of these steps may be challenging, resulting in significant hurdles to using machine learning. AutoML aims to simplify these steps for non-experts, and to make it easier for them to use machine learning techniques correctly and effectively.\\nAutoML plays an important role within the broader approach of automating data science, which also includes challenging tasks such as data engineering, data exploration and model interpretation and prediction.\\n\\n\\n== Targets of automation ==\\nAutomated machine learning can target various stages of the machine learning process.  Steps to automate are:\\n\\nData preparation and ingestion (from raw data and miscellaneous formats)\\nColumn type detection; e.g., Boolean, discrete numerical, continuous numerical, or text\\nColumn intent detection; e.g., target/label, stratification field, numerical feature, categorical text feature, or free text feature\\nTask detection; e.g., binary classification, regression, clustering, or ranking\\nFeature engineering\\nFeature selection\\nFeature extraction\\nMeta-learning and transfer learning\\nDetection and handling of skewed data and/or missing values\\nModel selection - choosing which machine learning algorithm to use, often including multiple competing software implementations\\nEnsembling - a form of consensus where using multiple models often gives better results than any single model\\nHyperparameter optimization of the learning algorithm and featurization\\nNeural architecture search\\nPipeline selection under time, memory, and complexity constraints\\nSelection of evaluation metrics and validation procedures\\nProblem checking\\nLeakage detection\\nMisconfiguration detection\\nAnalysis of obtained results\\nCreating user interfaces and visualizations\\n\\n\\n== Challenges and Limitations ==\\nThere are a number of key challenges being tackled around automated machine learning. A big issue surrounding the field is referred to as \"development as a cottage industry\". This phrase refers to the issue in machine learning where development relies on manual decisions and biases of experts. This is contrasted to the goal of machine learning which is to create systems that can learn and improve from their own usage and analysis of the data. Basically, it\\'s the struggle between how much experts should get involved in the learning of the systems versus how much freedom they should be giving the m'), Document(metadata={'title': 'Leakage (machine learning)', 'summary': \"In statistics and machine learning, leakage (also known as data leakage or target leakage) is the use of information in the model training process which would not be expected to be available at prediction time, causing the predictive scores (metrics) to overestimate the model's utility when run in a production environment.\\nLeakage is often subtle and indirect, making it hard to detect and eliminate. Leakage can cause a statistician or modeler to select a suboptimal model, which could be outperformed by a leakage-free model.\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/Leakage_(machine_learning)'}, page_content='In statistics and machine learning, leakage (also known as data leakage or target leakage) is the use of information in the model training process which would not be expected to be available at prediction time, causing the predictive scores (metrics) to overestimate the model\\'s utility when run in a production environment.\\nLeakage is often subtle and indirect, making it hard to detect and eliminate. Leakage can cause a statistician or modeler to select a suboptimal model, which could be outperformed by a leakage-free model.\\n\\n\\n== Leakage modes ==\\nLeakage can occur in many steps in the machine learning process. The leakage causes can be sub-classified into two possible sources of leakage for a model: features and training examples.\\n\\n\\n=== Feature leakage ===\\nFeature or column-wise leakage is caused by the inclusion of columns which are one of the following: a duplicate label, a proxy for the label, or the label itself. These features, known as anachronisms, will not be available when the model is used for predictions, and result in leakage if included when the model is trained.\\nFor example, including a \"MonthlySalary\" column when predicting \"YearlySalary\"; or \"MinutesLate\" when predicting \"IsLate\".\\n\\n\\n=== Training example leakage ===\\nRow-wise leakage is caused by improper sharing of information between rows of data. Types of row-wise leakage include:\\n\\nPremature featurization; leaking from premature featurization before Cross-validation/Train/Test split (must fit MinMax/ngrams/etc on only the train split, then transform the test set)\\nDuplicate rows between train/validation/test (e.g. oversampling a dataset to pad its size before splitting; e.g. different rotations/augmentations of a single image; bootstrap sampling before splitting; or duplicating rows to up sample the minority class)\\nNon-i.i.d. data\\nTime leakage (e.g. splitting a time-series dataset randomly instead of newer data in test set using a TrainTest split or rolling-origin cross validation)\\nGroup leakage—not including a grouping split column (e.g. Andrew Ng\\'s group had 100k x-rays of 30k patients, meaning ~3 images per patient. The paper used random splitting instead of ensuring that all images of a patient were in the same split. Hence the model partially memorized the patients instead of learning to recognize pneumonia in chest x-rays.)\\nA 2023 review found data leakage to be \"a widespread failure mode in machine-learning (ML)-based science\", having affected at least 294 academic publications across 17 disciplines, and causing a potential reproducibility crisis.\\n\\n\\n== Detection ==\\n\\n\\n== See also ==\\nAutoML\\nConcept drift (where the structure of the system being studied evolves over time, invalidating the model)\\nOverfitting\\nResampling (statistics)\\nSupervised learning\\nTraining, validation, and test sets\\n\\n\\n== References =='), Document(metadata={'title': 'Transduction (machine learning)', 'summary': 'In logic, statistical inference, and supervised learning,\\ntransduction or transductive inference is reasoning from\\nobserved, specific (training) cases to specific (test) cases. In contrast,\\ninduction is reasoning from observed training cases\\nto general rules, which are then applied to the test cases. The distinction is\\nmost interesting in cases where the predictions of the transductive model are\\nnot achievable by any inductive model. Note that this is caused by transductive\\ninference on different test sets producing mutually inconsistent predictions.\\nTransduction was introduced in a computer science context by Vladimir Vapnik in the 1990s, motivated by\\nhis view that transduction is preferable to induction since, according to him, induction requires\\nsolving a more general problem (inferring a function) before solving a more\\nspecific problem (computing outputs for new cases): \"When solving a problem of\\ninterest, do not solve a more general problem as an intermediate step. Try to\\nget the answer that you really need but not a more general one.\".\\nAn example of learning which is not inductive would be in the case of binary\\nclassification, where the inputs tend to cluster in two groups. A large set of\\ntest inputs may help in finding the clusters, thus providing useful information\\nabout the classification labels. The same predictions would not be obtainable\\nfrom a model which induces a function based only on the training cases.  Some\\npeople may call this an example of the closely related semi-supervised learning, since Vapnik\\'s motivation is quite different. \\nThe most well-known example of a case-bases learning algorithm is the k-nearest neighbor algorithm, which is related to transductive learning algorithms.\\nAnother example of an algorithm in this category is the Transductive Support Vector Machine (TSVM).\\nA third possible motivation of transduction arises through the need\\nto approximate. If exact inference is computationally prohibitive, one may at\\nleast try to make sure that the approximations are good at the test inputs. In\\nthis case, the test inputs could come from an arbitrary distribution (not\\nnecessarily related to the distribution of the training inputs), which wouldn\\'t\\nbe allowed in semi-supervised learning. An example of an algorithm falling in\\nthis category is the Bayesian Committee Machine (BCM).\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Transduction_(machine_learning)'}, page_content='In logic, statistical inference, and supervised learning,\\ntransduction or transductive inference is reasoning from\\nobserved, specific (training) cases to specific (test) cases. In contrast,\\ninduction is reasoning from observed training cases\\nto general rules, which are then applied to the test cases. The distinction is\\nmost interesting in cases where the predictions of the transductive model are\\nnot achievable by any inductive model. Note that this is caused by transductive\\ninference on different test sets producing mutually inconsistent predictions.\\nTransduction was introduced in a computer science context by Vladimir Vapnik in the 1990s, motivated by\\nhis view that transduction is preferable to induction since, according to him, induction requires\\nsolving a more general problem (inferring a function) before solving a more\\nspecific problem (computing outputs for new cases): \"When solving a problem of\\ninterest, do not solve a more general problem as an intermediate step. Try to\\nget the answer that you really need but not a more general one.\".\\nAn example of learning which is not inductive would be in the case of binary\\nclassification, where the inputs tend to cluster in two groups. A large set of\\ntest inputs may help in finding the clusters, thus providing useful information\\nabout the classification labels. The same predictions would not be obtainable\\nfrom a model which induces a function based only on the training cases.  Some\\npeople may call this an example of the closely related semi-supervised learning, since Vapnik\\'s motivation is quite different. \\nThe most well-known example of a case-bases learning algorithm is the k-nearest neighbor algorithm, which is related to transductive learning algorithms.\\nAnother example of an algorithm in this category is the Transductive Support Vector Machine (TSVM).\\nA third possible motivation of transduction arises through the need\\nto approximate. If exact inference is computationally prohibitive, one may at\\nleast try to make sure that the approximations are good at the test inputs. In\\nthis case, the test inputs could come from an arbitrary distribution (not\\nnecessarily related to the distribution of the training inputs), which wouldn\\'t\\nbe allowed in semi-supervised learning. An example of an algorithm falling in\\nthis category is the Bayesian Committee Machine (BCM).\\n\\n\\n== Historical Context ==\\nThe mode of inference from particulars to particulars, which Vapnik came to call transduction, was already distinguished from the mode of inference from particulars to generalizations in part III of the Cambridge philosopher and logician W.E. Johnson\\'s 1924 textbook, Logic. In Johnson\\'s work, the former mode was called \\'eduction\\' and the latter was called \\'induction\\'. Bruno de Finetti developed a purely subjective form of Bayesianism in which claims about objective chances could be translated into empirically respectable claims about subjective credences with respect to observables through exchangeability properties. An early statement of this view can be found in his 1937 La Prévision: ses Lois Logiques, ses Sources Subjectives and a mature statement in his 1970 Theory of Probability. Within de Finetti\\'s subjective Bayesian framework, all inductive inference is ultimately inference from particulars to particulars. \\n\\n\\n== Example problem ==\\nThe following example problem contrasts some of the unique properties of transduction against induction.\\n\\nA collection of points is given, such that some of the points are labeled (A, B, or C), but most of the points are unlabeled (?). The goal is to predict appropriate labels for all of the unlabeled points.\\nThe inductive approach to solving this problem is to use the labeled points to train a supervised learning algorithm, and then have it predict labels for all of the unlabeled points. With this problem, however, the supervised learning algorithm will only have five labeled points to use as a basis for building a predictive model. It will certainly struggle to bui'), Document(metadata={'title': 'Deep learning', 'summary': 'Deep learning is a subset of machine learning methods based on neural networks with representation learning. The field takes inspiration from biological neuroscience and is centered around stacking artificial neurons into layers and \"training\" them to process data. The adjective \"deep\" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be either supervised, semi-supervised or unsupervised.\\nSome common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\\nEarly forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose.', 'source': 'https://en.wikipedia.org/wiki/Deep_learning'}, page_content='Deep learning is a subset of machine learning methods based on neural networks with representation learning. The field takes inspiration from biological neuroscience and is centered around stacking artificial neurons into layers and \"training\" them to process data. The adjective \"deep\" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be either supervised, semi-supervised or unsupervised.\\nSome common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\\nEarly forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose.\\n\\n\\n== Overview ==\\nMost modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.\\nFundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a slightly more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face.\\nImportantly, a deep learning process can learn which features to optimally place at which level on its own. Prior to deep learning, machine learning techniques often involved hand-crafted feature engineering to transform the data into a more suitable representation for a classification algorithm to operate on. In the deep learning approach, features are not hand-crafted and the model discovers useful feature representations from the data automatically. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.\\nThe word \"deep\" in \"deep learning\" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited. No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than two. CAP of depth two has been shown to be a universal approximator in the sense that it can emulate any function. Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > two) are able to extract better features than shallow models and he'), Document(metadata={'title': 'Machine learning control', 'summary': 'Machine learning control (MLC) is a subfield of machine learning, intelligent control and control theory\\nwhich solves optimal control problems with methods of machine learning.\\nKey applications are complex nonlinear systems\\nfor which linear control theory methods are not applicable.', 'source': 'https://en.wikipedia.org/wiki/Machine_learning_control'}, page_content='Machine learning control (MLC) is a subfield of machine learning, intelligent control and control theory\\nwhich solves optimal control problems with methods of machine learning.\\nKey applications are complex nonlinear systems\\nfor which linear control theory methods are not applicable.\\n\\n\\n== Types of problems and tasks ==\\nFour types of problems are commonly encountered.\\n\\nControl parameter identification: MLC translates to a parameter identification if the structure of the control law is given but the parameters are unknown. One example is the genetic algorithm for optimizing coefficients of a PID controller or discrete-time optimal control.\\nControl design as regression problem of the first kind:  MLC approximates a general nonlinear mapping from sensor signals to actuation commands, if the sensor signals and the optimal actuation command are known for every state. One example is the computation of sensor feedback from a known full state feedback. A neural network is commonly used technique for this task.\\nControl design as regression problem of the second kind: MLC may also identify arbitrary nonlinear control laws which minimize the cost function of the plant. In this case, neither a model, nor the control law structure,  nor the optimizing actuation command needs to be known. The optimization is only based on the control performance (cost function) as measured in the plant. Genetic programming is a powerful regression technique for this purpose.\\nReinforcement learning control: The control law may be continually updated over measured performance changes (rewards) using reinforcement learning.\\nMLC comprises, for instance, neural network control, \\ngenetic algorithm based control, \\ngenetic programming control,\\nreinforcement learning control, \\nand has methodological overlaps with other data-driven control,\\nlike artificial intelligence and robot control.\\n\\n\\n== Applications ==\\nMLC has been successfully applied\\nto many nonlinear control problems,\\nexploring unknown and often unexpected actuation mechanisms.\\nExample applications include\\n\\nAttitude control of satellites.\\nBuilding thermal control.\\nFeedback turbulence control.\\nRemotely operated underwater vehicles.\\nMany more engineering MLC application are summarized in the review article of PJ Fleming & RC Purshouse (2002).\\nAs for all general nonlinear methods,\\nMLC comes with no guaranteed convergence, \\noptimality or robustness for a range of operating conditions.\\n\\n\\n== References ==\\n\\n\\n== Further reading =='), Document(metadata={'title': 'Tensor (machine learning)', 'summary': 'Tensor informally refers in machine learning to two different concepts that organize and represent data. Data may be organized in a multidimensional array (M-way array) that is informally referred to as a \"data tensor\";  however in the strict mathematical sense, a tensor is a multilinear mapping over a set of domain vector spaces to a range vector space.  Observations, such as images, movies, volumes, sounds, and relationships among words and concepts, stored in an M-way array (\"data tensor\") may be analyzed either by artificial neural networks or tensor methods.\\nTensor decomposition can factorize data tensors into smaller tensors. Operations on data tensors can be expressed in terms of matrix multiplication and the Kronecker product.  The computation of gradients, an important aspect of the backpropagation algorithm, can be performed using PyTorch and TensorFlow.\\nComputations are often performed on graphics processing units (GPUs) using CUDA and on dedicated hardware such as Google\\'s Tensor Processing Unit or Nvidia\\'s Tensor core. These developments have greatly accelerated neural network architectures and increased the size and complexity of models that can be trained.', 'source': 'https://en.wikipedia.org/wiki/Tensor_(machine_learning)'}, page_content='Tensor informally refers in machine learning to two different concepts that organize and represent data. Data may be organized in a multidimensional array (M-way array) that is informally referred to as a \"data tensor\";  however in the strict mathematical sense, a tensor is a multilinear mapping over a set of domain vector spaces to a range vector space.  Observations, such as images, movies, volumes, sounds, and relationships among words and concepts, stored in an M-way array (\"data tensor\") may be analyzed either by artificial neural networks or tensor methods.\\nTensor decomposition can factorize data tensors into smaller tensors. Operations on data tensors can be expressed in terms of matrix multiplication and the Kronecker product.  The computation of gradients, an important aspect of the backpropagation algorithm, can be performed using PyTorch and TensorFlow.\\nComputations are often performed on graphics processing units (GPUs) using CUDA and on dedicated hardware such as Google\\'s Tensor Processing Unit or Nvidia\\'s Tensor core. These developments have greatly accelerated neural network architectures and increased the size and complexity of models that can be trained.\\n\\n\\n== History ==\\nA tensor is by definition a multilinear map. In mathematics, this may express a multilinear relationship between sets of algebraic objects. In physics, tensor fields, considered as tensors at each point in space, are useful in expressing mechanics such as stress or elasticity. In machine learning, the exact use of tensors depends on the statistical approach being used.\\nIn 2001, the field of signal processing and statistics were making use of tensor methods. Pierre Comon surveys the early adoption of tensor methods in the fields of telecommunications, radio surveillance, chemometrics and sensor processing. Linear tensor rank methods (such as, Parafac/CANDECOMP) analyzed M-way arrays (\"data tensors\") composed of higher order statistics that were employed in blind source separation problems to compute a linear model of the data. He noted several early limitations in determining the tensor rank and efficient tensor rank decomposition.\\nIn the early 2000s, multilinear tensor methods crossed over into computer vision, computer graphics and machine learning with papers by Vasilescu  or in collaboration with Terzopoulos, such as Human Motion Signatures, TensorFaces  TensorTexures and Multilinear Projection.  Multilinear algebra, the algebra of higher-order tensors, is a suitable and transparent framework for analyzing the multifactor structure of an ensemble of observations and for addressing the difficult problem of disentangling the causal factors based on second order or higher order statistics associated with each causal factor.  \\nTensor (multilinear) factor analysis disentangles and reduces the influence of different causal factors with multilinear subspace learning.  \\nWhen treating an image or a video as a 2- or 3-way array, i.e., \"data matrix/tensor\",  tensor methods reduce spatial or time redundancies as demonstrated by Wang and Ahuja.\\nYoshua Bengio,\\nGeoff Hinton\\n and their collaborators briefly discuss the relationship between deep neural networks \\nand tensor factor analysis beyond the use of M-way arrays (\"data tensors\") as inputs.   One of the early uses of tensors for neural networks appeared in natural language processing. A single word can be expressed as a vector via Word2vec. Thus a relationship between two words can be encoded in a matrix. However, for more complex relationships such as subject-object-verb, it is necessary to build higher-dimensional networks. In 2009, the work of Sutskever introduced Bayesian Clustered Tensor Factorization to model relational concepts while reducing the parameter space. From 2014 to 2015, tensor methods become more common in convolutional neural networks (CNNs). Tensor methods organize neural network weights in a \"data tensor\", analyze and reduce the number of neural network weights.  Lebedev et al. ac'), Document(metadata={'title': 'Online machine learning', 'summary': 'In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g., stock price prediction. Online learning algorithms may be prone to catastrophic interference, a problem that can be addressed by incremental learning approaches.', 'source': 'https://en.wikipedia.org/wiki/Online_machine_learning'}, page_content='In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g., stock price prediction. Online learning algorithms may be prone to catastrophic interference, a problem that can be addressed by incremental learning approaches.\\n\\n\\n== Introduction ==\\nIn the setting of supervised learning, a function of \\n  \\n    \\n      \\n        f\\n        :\\n        X\\n        →\\n        Y\\n      \\n    \\n    {\\\\displaystyle f:X\\\\to Y}\\n  \\n is to be learned, where \\n  \\n    \\n      \\n        X\\n      \\n    \\n    {\\\\displaystyle X}\\n  \\n is thought of as a space of inputs and \\n  \\n    \\n      \\n        Y\\n      \\n    \\n    {\\\\displaystyle Y}\\n  \\n as a space of outputs, that predicts well on instances that are drawn from a joint probability distribution \\n  \\n    \\n      \\n        p\\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle p(x,y)}\\n  \\n on \\n  \\n    \\n      \\n        X\\n        ×\\n        Y\\n      \\n    \\n    {\\\\displaystyle X\\\\times Y}\\n  \\n. In reality, the learner never knows the true distribution \\n  \\n    \\n      \\n        p\\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle p(x,y)}\\n  \\n over instances. Instead, the learner usually has access to a training set of examples \\n  \\n    \\n      \\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            1\\n          \\n        \\n        )\\n        ,\\n        …\\n        ,\\n        (\\n        \\n          x\\n          \\n            n\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            n\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (x_{1},y_{1}),\\\\ldots ,(x_{n},y_{n})}\\n  \\n. In this setting, the loss function is given as \\n  \\n    \\n      \\n        V\\n        :\\n        Y\\n        ×\\n        Y\\n        →\\n        \\n          R\\n        \\n      \\n    \\n    {\\\\displaystyle V:Y\\\\times Y\\\\to \\\\mathbb {R} }\\n  \\n, such that \\n  \\n    \\n      \\n        V\\n        (\\n        f\\n        (\\n        x\\n        )\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle V(f(x),y)}\\n  \\n measures the difference between the predicted value \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle f(x)}\\n  \\n and the true value \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n  \\n. The ideal goal is to select a function \\n  \\n    \\n      \\n        f\\n        ∈\\n        \\n          \\n            H\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle f\\\\in {\\\\mathcal {H}}}\\n  \\n, where \\n  \\n    \\n      \\n        \\n          \\n            H\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {H}}}\\n  \\n is a space of functions called a hypothesis space, so that some notion of total loss is minimized. Depending on the type of model (statistical or adversarial), one can devise different notions of loss, which lead to different learning algorithms.\\n\\n\\n== Statistical view of online learning ==\\nIn statistical learning models, the training sample \\n  \\n    \\n      \\n        (\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            i\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (x_{i},y_{i})}\\n  \\n are assumed to have been drawn from the true distribution \\n  \\n    \\n      \\n        p\\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle p(x,y)}\\n  \\n and the objective is to minimize the expected \"risk\"\\n'), Document(metadata={'title': 'Supervised learning', 'summary': 'Supervised learning (SL) is a paradigm in machine learning where input objects (for example, a vector of predictor variables) and a desired output value (also known as a human-labeled supervisory signal) train a model. The training data is processed, building a function that maps new data to expected output values. An optimal scenario will allow for the algorithm to correctly determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Supervised_learning'}, page_content='Supervised learning (SL) is a paradigm in machine learning where input objects (for example, a vector of predictor variables) and a desired output value (also known as a human-labeled supervisory signal) train a model. The training data is processed, building a function that maps new data to expected output values. An optimal scenario will allow for the algorithm to correctly determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error.\\n\\n\\n== Steps to follow ==\\nTo solve a given problem of supervised learning, one has to perform the following steps:\\n\\nDetermine the type of training examples. Before doing anything else, the user should decide what kind of data is to be used as a training set. In the case of handwriting analysis, for example, this might be a single handwritten character, an entire handwritten word, an entire sentence of handwriting or perhaps a full paragraph of handwriting.\\nGather a training set. The training set needs to be representative of the real-world use of the function. Thus, a set of input objects is gathered and corresponding outputs are also gathered, either from human experts or from measurements.\\nDetermine the input feature representation of the learned function. The accuracy of the learned function depends strongly on how the input object is represented. Typically, the input object is transformed into a feature vector, which contains a number of features that are descriptive of the object. The number of features should not be too large, because of the curse of dimensionality; but should contain enough information to accurately predict the output.\\nDetermine the structure of the learned function and corresponding learning algorithm. For example, the engineer may choose to use support-vector machines or decision trees.\\nComplete the design. Run the learning algorithm on the gathered training set. Some supervised learning algorithms require the user to determine certain control parameters. These parameters may be adjusted by optimizing performance on a subset (called a validation set) of the training set, or via cross-validation.\\nEvaluate the accuracy of the learned function. After parameter adjustment and learning, the performance of the resulting function should be measured on a test set that is separate from the training set.\\n\\n\\n== Algorithm choice ==\\nA wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the No free lunch theorem).\\nThere are four major issues to consider in supervised learning:\\n\\n\\n=== Bias-variance tradeoff ===\\n\\nA first issue is the tradeoff between bias and variance. Imagine that we have available several different, but equally good, training data sets. A learning algorithm is biased for a particular input \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n. A learning algorithm has high variance for a particular input \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n if it predicts different output values when trained on different training sets. The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm. Generally, there is a tradeoff between bias and variance. A learning algorithm with low bias must be \"flexible\" so that it can fit the data well. But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance. A key aspect of many supervised learning methods is that they are able to adjust this tradeoff bet'), Document(metadata={'title': 'Torch (machine learning)', 'summary': 'Torch is an open-source machine learning library, \\na scientific computing framework, and a scripting language based on Lua. It provides LuaJIT interfaces to deep learning algorithms implemented in C. It was created by the Idiap Research Institute at EPFL. Torch development moved in 2017 to PyTorch, a port of the library to Python.', 'source': 'https://en.wikipedia.org/wiki/Torch_(machine_learning)'}, page_content='Torch is an open-source machine learning library, \\na scientific computing framework, and a scripting language based on Lua. It provides LuaJIT interfaces to deep learning algorithms implemented in C. It was created by the Idiap Research Institute at EPFL. Torch development moved in 2017 to PyTorch, a port of the library to Python.\\n\\n\\n== torch ==\\nThe core package of Torch is torch. It provides a flexible N-dimensional array or Tensor, which supports basic routines for indexing, slicing, transposing, type-casting, resizing, sharing storage and cloning. This object is used by most other packages and thus forms the core object of the library. The Tensor also supports mathematical operations like max, min, sum, statistical distributions like uniform, normal and multinomial, and BLAS operations like dot product, matrix–vector multiplication, matrix–matrix multiplication  and matrix product.\\nThe following exemplifies using torch via its REPL interpreter:\\n\\nThe torch package also simplifies object-oriented programming and serialization by providing various convenience functions which are used throughout its packages. The torch.class(classname, parentclass) function can be used to create object factories (classes). When the constructor is called, torch initializes and sets a Lua table with the user-defined metatable, which makes the table an object.\\nObjects created with the torch factory can also be serialized, as long as they do not contain references to objects that cannot be serialized, such as Lua coroutines, and Lua userdata. However, userdata can be serialized if it is wrapped by a table (or metatable) that provides  read() and write() methods.\\n\\n\\n== nn ==\\nThe nn package is used for building neural networks. It is divided into modular objects that share a common Module interface. Modules have a forward() and backward() method that allow them to feedforward and backpropagate, respectively. Modules can be joined using module composites, like Sequential, Parallel and Concat to create complex task-tailored graphs. Simpler modules like Linear, Tanh and Max make up the basic component modules. This modular interface provides first-order automatic gradient differentiation. What follows is an example use-case for building a multilayer perceptron using Modules:\\n\\nLoss functions are implemented as sub-classes of Criterion, which has a similar interface to Module. It also has forward() and backward() methods for computing the loss and backpropagating gradients, respectively. Criteria are helpful to train neural network on classical tasks. Common criteria are the mean squared error criterion implemented in MSECriterion and the cross-entropy criterion implemented in ClassNLLCriterion. What follows is an example of a Lua function that can be iteratively called to train \\nan mlp Module on input Tensor x, target Tensor y with a scalar learningRate:    \\n\\nIt also has StochasticGradient class for training a neural network using stochastic gradient descent, although the optim package provides much more options in this respect, like momentum and weight decay regularization.\\n\\n\\n== Other packages ==\\nMany packages other than the above official packages are used with Torch. These are listed in the torch cheatsheet. These extra packages provide a wide range of utilities such as parallelism, asynchronous input/output, image processing, and so on. They can be installed with LuaRocks, the Lua package manager which is also included with the Torch distribution.\\n\\n\\n== Applications ==\\nTorch is used by the Facebook AI Research Group, IBM, Yandex and the Idiap Research Institute. Torch has been extended for use on Android and iOS. It has been used to build hardware implementations for data flows like those found in neural networks.\\nFacebook has released a set of extension modules as open source software.\\n\\n\\n== See also ==\\nComparison of deep learning software\\nPyTorch\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nOfficial website'), Document(metadata={'title': 'Fairness (machine learning)', 'summary': \"Fairness in machine learning refers to the various attempts at correcting algorithmic bias in automated decision processes based on machine learning models. Decisions made by computers after a machine-learning process may be considered unfair if they were based on variables considered sensitive. For example gender, ethnicity, sexual orientation or disability. As it is the case with many ethical concepts, definitions of fairness and bias are always controversial. In general, fairness and bias are considered relevant when the decision process impacts people's lives. In machine learning, the problem of algorithmic bias is well known and well studied. Outcomes may be skewed by a range of factors and thus might be considered unfair with respect to certain groups or individuals. An example would be the way social media sites deliver personalized news to consumer.\", 'source': 'https://en.wikipedia.org/wiki/Fairness_(machine_learning)'}, page_content='Fairness in machine learning refers to the various attempts at correcting algorithmic bias in automated decision processes based on machine learning models. Decisions made by computers after a machine-learning process may be considered unfair if they were based on variables considered sensitive. For example gender, ethnicity, sexual orientation or disability. As it is the case with many ethical concepts, definitions of fairness and bias are always controversial. In general, fairness and bias are considered relevant when the decision process impacts people\\'s lives. In machine learning, the problem of algorithmic bias is well known and well studied. Outcomes may be skewed by a range of factors and thus might be considered unfair with respect to certain groups or individuals. An example would be the way social media sites deliver personalized news to consumer.\\n\\n\\n== Context ==\\nDiscussion about fairness in machine learning is a relatively recent topic. Since 2016 there has been a sharp increase in research into the topic. This increase could be partly accounted to an influential report by ProPublica that claimed that the COMPAS software, widely used in US courts to predict recidivism, was racially biased. One topic of research and discussion is the definition of fairness, as there is no universal definition, and different definitions can be in contradiction with each other, which makes it difficult to judge machine learning models. Other research topics include the origins of bias, the types of bias, and methods to reduce bias.\\nIn recent years tech companies have made tools and manuals on how to detect and reduce bias in machine learning. IBM has tools for Python and R with several algorithms to reduce software bias and increase its fairness. Google has published guidelines and tools to study and combat bias in machine learning. Facebook have reported their use of a tool, Fairness Flow, to detect bias in their AI. However, critics have argued that the company\\'s efforts are insufficient, reporting little use of the tool by employees as it cannot be used for all their programs and even when it can, use of the tool is optional.\\nIt is important to note that the discussion about quantitative ways to test fairness and unjust discrimination in decision-making predates by several decades the rather recent debate on fairness in machine learning. In fact, a vivid discussion of this topic by the scientific community flourished during the mid-1960s and 1970s, mostly as a result of the American civil rights movement and, in particular, of the passage of the U.S. Civil Rights Act of 1964. However, by the end of the 1970s, the debate largely disappeared, as the different and sometimes competing notions of fairness left little room for clarity on when one notion of fairness may be preferable to another.\\n\\n\\n=== Language Bias ===\\nLanguage bias refers a type of statistical sampling bias tied to the language of a query that leads to \"a systematic deviation in sampling information that prevents it from accurately representing the true coverage of topics and views available in their repository.\" Luo et al. show that current large language models, as they are predominately trained on English-language data, often present the Anglo-American views as truth, while systematically downplaying non-English perspectives as irrelevant, wrong, or noise. When queried with political ideologies like \"What is liberalism?\", ChatGPT, as it was trained on English-centric data, describes liberalism from the Anglo-American perspective, emphasizing aspects of human rights and equality, while equally valid aspects like \"opposes state intervention in personal and economic life\" from the dominant Vietnamese perspective and \"limitation of government power\" from the prevalent Chinese perspective are absent. Similarly, other political perspectives embedded in Japanese, Korean, French, and German corpora are absent in ChatGPT\\'s responses. ChatGPT, covered itself as a multilingual ch')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "# Load data from Wikipedia given a topic\n",
    "loader = WikipediaLoader(\"Machine_learning\")\n",
    "document = loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There's also some loaders to handle propietary sources, like `MongodbLoader`, for loading documents from a MongoDB database.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `document transformers` are tools to fetch only the relevant details of documents. This involves several steps : \n",
    "\n",
    "The first one is divide the text in chunks. It's important that in the chunking process, the data segments generated are smaller than the maximum input size of the LLM that is going to be used. Langchain offers this 3 chunking strategies : \n",
    "\n",
    "-   **Fixed-size chunks** : Define a fixed size. Allows some overlap between chunks. Overlapping ensures continuity and context preservation, improving accuracy and coherence of the generated chunks. One class to execute this strategy is `CharacterTextSplitter`.\n",
    "-   **Variable-sized chunks** : Chunks based on content characteristics, like end-of-sentence. It ensures the preservation of coherent and contextually intact of the content of each chunk. This strategy can be executed with `RecursiveCharacterTextSplitter`.\n",
    "-   **Customized chunking** : When dealing with large documents, is recommended to use variable-sized chunks but also append the document title to chunks from tho the middle of the document to prevent content loss. This can be done with `MarkdownHeaderTextSplitter`.\n",
    "\n",
    "So, as summary, chunking provides you the capacity of divide a large document into small documents that can be processed by the selected LLM model. However, the main drawback of this strategy is the possibility of losing context information of each segment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process means storing and organizing data from various sources into a vector store. This process consists of storing chunk along with an embedding representation of it, which captures the meaning of it's text and makes easy to retrieve/select a chunk by semantic similarity. This embedding are generated by embedding models, such as `OpenAIEmbeddings` models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, **we will create an instance of a Ollama model**. Please, follow these instructions to install all the dependencies required : [link](https://python.langchain.com/docs/integrations/llms/ollama/).\n",
    "\n",
    "**Note** : You have to run the following command in your computer to install the embedding model used in the following code : `ollama run llama3.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The capital of Australia is Canberra. Would you like to know more about Canberra or Australia in general?' additional_kwargs={} response_metadata={'model': 'llama3.1:8b', 'created_at': '2024-10-02T15:13:02.376106Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 794369400, 'load_duration': 69445000, 'prompt_eval_count': 28, 'prompt_eval_duration': 277413000, 'eval_count': 21, 'eval_duration': 443885000} id='run-7b824c01-b40c-4e99-84e8-572b53c0355f-0' usage_metadata={'input_tokens': 28, 'output_tokens': 21, 'total_tokens': 49}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "chat = ChatOllama(model=\"llama3.1:8b\", temperature=0.1) # Load the chat model\n",
    "messages = [ # Create a context description for the chat\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful assistant.\"\n",
    "    ), # Create an input from a human\n",
    "    HumanMessage(\n",
    "        content=\"What is the capital of Australia?\"\n",
    "    )\n",
    "]\n",
    "\n",
    "response = chat.invoke(messages) # Get an AI message based on the HumanMessage and SystemMessage\n",
    "print(response)\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's 3 types of messages in Langchain : \n",
    "\n",
    "-   **SystemMessage** : Sets the behaviour of the chat model, giving a context and instructions to the model. \n",
    "-   **HumanMessage** : To store the user's prompts sent to the model.\n",
    "-   **AIMessage** : Represent the responses of the model.\n",
    "\n",
    "It should be highlighted that **HumanMessage** and **AIMessage** can be used to create an history about the conversation that the chat can use in the next interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding models are models that transform text into vector representations, enabling operations like semantic search through text similaruty in vector space. The `embed_method()` is the most used method to embed multiple texts, providing a list of vector representations.\n",
    "\n",
    "**Note** : You have to run the following command in your computer to install the embedding model used in the following code : `ollama run llama3.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents embedded : 3\n",
      "The length of each embedding is : 4096\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Create the embedding model\n",
    "embedding_model = OllamaEmbeddings(model=\"llama3.1\")\n",
    "\n",
    "# Embed a list of texts\n",
    "embeddings = embedding_model.embed_documents(\n",
    "    [\"Hi there!\", \"Oh, hello!\", \"What's your name?\"]\n",
    ")\n",
    "\n",
    "print(f\"Number of documents embedded : {len(embeddings)}\")\n",
    "print(f\"The length of each embedding is : {len(embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, each phrase/text would be stored into a embedding of 4096 components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of embeddings for indexing and giving a context for each chunk obtained requires the creation of databases for storing and searching effectively these embeddings.\n",
    "\n",
    "Traditional databases are not optimized for this purpose. However, vector stores are built to handle this type of data. They offer speed, scalability and precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Retrievers` are interfaces that return documents in response to a query. The most straightforward approach is to use basic similarity metrics like the `cosine similarity`. This method compares the embeddings of the question and potential answers, ensuring that the response generated is semantically aligned with the query.This method narrows down the most contextually relevant information from a vast dataset, improving the precision and relevance of the responses. In other words, measures the similarity between the user's prompt and the documents.\n",
    "\n",
    "However, Langchain offers more advanced methods like : \n",
    "\n",
    "-   **Parent Document Retriever** : Instead of working with a whole documents, it uses the chunks generated from all the documents and the embeddings generated. This methods extracts the chunk with the highest similarity with the prompt, and give the parent document of that chunk to the model as context information.\n",
    "-   **Self-Query Retriever** : This method not only uses semanthical data, but also generates filters based on the user prompt's and the metadata of the documents. Sometimes user's prompt contains some logical information that can be represented as metadata filters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A chain in Langchain is a harcoded union of different elements to perform a task. Each component of the chain uses the output of the previous component in the chain as input. There are two types of chains : **LLMChain** and **SequentialChain**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chain transforms the user's prompt using a `PromptTemplate`, an object that can interact with the model. This objects accepts a string, the user question, as well as other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleja\\AppData\\Local\\Temp\\ipykernel_25996\\3643495191.py:12: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(prompt=prompt, llm=chat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the colors of the rainbow, in order:\n",
      "\n",
      "1. **Red**\n",
      "2. **Orange**\n",
      "3. **Yellow**\n",
      "4. **Green**\n",
      "5. **Blue**\n",
      "6. **Indigo**\n",
      "7. **Violet**\n",
      "\n",
      "You can also remember them using the acronym ROYGBIV!\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chat = ChatOllama(model=\"llama3.1:8b\", temperature=0.1) # Create the chat model\n",
    "template = \"List all the color in the rainbow.\" # Create the template/user's message\n",
    "prompt = PromptTemplate(template=template, # User's question\n",
    "                        input_variables=[], # There's no input variables\n",
    "                        output_parser=StrOutputParser() # Parser to give a string format to the output\n",
    "                        )\n",
    "chain = LLMChain(prompt=prompt, llm=chat)\n",
    "response = chain.predict()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the previous warning, this method is deprecated. Now the most common implementation is the **LangChain Expression Language(LCEL)** scheme to make code more readable. It works exactly as the previous option, but it's easier to understand hwo it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the 7 colors of a rainbow, in order:\n",
      "\n",
      "1. **Red**\n",
      "2. **Orange**\n",
      "3. **Yellow**\n",
      "4. **Green**\n",
      "5. **Blue**\n",
      "6. **Indigo**\n",
      "7. **Violet**\n",
      "\n",
      "You can also remember them using the acronym ROYGBIV!\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"List all the colors in a {item}\") # We define the prompt, but in this case there's the input variable 'item'\n",
    "chat = ChatOllama(model=\"llama3.1:8b\", temperature=0.1) # Create the chat model\n",
    "# Generate the chain with this new format\n",
    "chain = prompt | chat | StrOutputParser()\n",
    "response = chain.invoke({\"item\":\"rainbow\"}) # Invoke the chain with the input variable 'item' equals to 'rainbow'\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Sequential Chain it's a more advanced structure which allows to make a series of subsequent call to an LLM. This structure is useful for using an output of a call as input for the next call. It allows streamlining the process, as well as enables more complex interactions across various applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Social Media Post Review**\n",
      "\n",
      "**Overall Score: 8/10**\n",
      "\n",
      "**Strengths:**\n",
      "\n",
      "1. **Clear Call-to-Action (CTA)**: The post clearly communicates the Black Friday sale details, including the discount percentage and the code to use at checkout.\n",
      "2. **Emotional Engagement**: The use of phrases like \"SHOP 'TIL YOU DROP!\" and \"Hurry, this offer won't last long!\" creates a sense of urgency and excitement, encouraging users to take action.\n",
      "3. **Visual Hierarchy**: The bold font and capital letters used for the title (\"BLACK FRIDAY ALERT!\") effectively grab attention and make the post stand out in a crowded feed.\n",
      "\n",
      "**Weaknesses:**\n",
      "\n",
      "1. **Lack of Personalization**: While the post thanks customers, it feels somewhat generic and doesn't address individual users by name or reference their specific interests.\n",
      "2. **Overemphasis on Discounts**: The focus solely on discounts might lead to a \"deals-only\" mentality among customers, potentially undermining the brand's value proposition.\n",
      "3. **Limited Engagement Opportunities**: The post doesn't encourage users to share their experiences, ask questions, or engage with others in the comments section.\n",
      "\n",
      "**Suggestions for Improvement:**\n",
      "\n",
      "1. **Add Personal Touches**: Incorporate user-generated content, customer testimonials, or personalized messages to create a stronger emotional connection with customers.\n",
      "2. **Highlight Brand Value**: Balance the emphasis on discounts by highlighting the brand's unique value proposition, such as high-quality products, excellent customer service, or exclusive experiences.\n",
      "3. **Encourage Engagement**: Add questions, polls, or interactive elements to encourage users to engage with the post and each other.\n",
      "\n",
      "**Target Audience:**\n",
      "\n",
      "* Demographics: Women and men aged 25-45\n",
      "* Interests: Shopping, fashion, beauty, home decor, and lifestyle enthusiasts\n",
      "* Psychographics: People seeking exclusive deals, discounts, and experiences\n",
      "\n",
      "By addressing these areas for improvement, you can enhance the post's effectiveness in driving sales, engagement, and brand loyalty.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "post_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are a business owner. Given the theme of a post, write a social media post to share on my socials.\n",
    "    \n",
    "    Theme : {theme}\n",
    "    Content : This is social media post based on the theme above:\n",
    "    \"\"\"\n",
    "\n",
    ")\n",
    "review_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an expert social media manager. Given the presented social media post, it is your job to write a review for the post.\n",
    "\n",
    "    Social media post : \n",
    "    {post}\n",
    "    Review from a Social Media Expert:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1:8b\", temperature=0.1)\n",
    "chain = (\n",
    "    {\"post\" : post_prompt | llm | StrOutputParser()} # LLM generates an output with string format from the 'post_prompt' --> Generates the social media post.\n",
    "    | review_prompt # Review prompt\n",
    "    | llm # Call again the LLM --> Generates the evaluation of the social media post.\n",
    "    | StrOutputParser() # Generates an output with string format\n",
    ")\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"theme\" : \"Having a black friday sale with 50% off on everything.\"\n",
    "    }\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory is the key point on maintaining context on dialogues. Langchain's memory allows to store input and output messages in a structured manner. With memoty the system can offer more personalized and relevant information by remembering and referring to past interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGCourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
