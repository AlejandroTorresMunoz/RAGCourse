{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LlamaIndex** is a framework to connect data and LLM's. This data will be loaded into a some type of structure that later will receive the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of RAG and it's components with LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective of retrieval augmentation is to put some context to the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way RAG works is \n",
    "\n",
    "1.  A documents is loaded and divided into chunks. This chunks processed by a embedding model .Finally,  their vector representations are stored into a vector database. **This first step is the data ingestion**.\n",
    "2.  **The second step is data querying(retrieval+synthesis)**. At this step, chunks of data are extracted from the vector database, based on the similarity with the user's prompt, and given as context to the LLM. You can extract the l-most similar chunks from the vector database and plug them to the synthesis module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the main component's in this framework are these : \n",
    "\n",
    "-   *LlamaHub (Data ingestion)* : Connect to your existing data, like PDF's, doc's, DDBB's...\n",
    "-   *Data Structures* : Store and index your data for different use cases. It can be integrated with different DDBB's, like vector db.\n",
    "-   *Queries* : Retrieve and query over the stored data in the data structures. This includes agents, QA, summarization, ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector store databases enable to store high-dimensional data and provide the essential tools for semantically retrieving relevant documents. These systems analyze the emebddings vectors that encapsulate the entire document's meaning.\n",
    "\n",
    "A primary function is the similarity search. Semantic search transcends traditional keyword matching. It captures the meaning in vectorized representations, and this technique can be applied to all data formats. Once we have the embedded format, we can calculate indexed similarities or capture the context embedded in the query. These ensures that the results are relevant and in line with the contextual and conceptual nuances of the user input's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Connectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Managing data in diverse formats can be challenging, like PDF's, doc's, DDBB's, .csv's... To solve this problem we use the data connectors, also called `Readers`. Readers are responsible for parsing and converting the data into a simplified `Document` representation, **consisting in text and basic metadata**.\n",
    "\n",
    "So, in summary, data connectors are designed to to streamline the data ingestion process, automating the process of fetching data fro differents sources and format it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleja\\AppData\\Local\\Temp\\ipykernel_5740\\2867412336.py:3: DeprecationWarning: Call to deprecated function (or staticmethod) download_loader. (`download_loader()` is deprecated. Please install tool using pip install directly instead.)\n",
      "  WikipediaReader = download_loader(\"WikipediaReader\") # Download the wikipedia reader to fetch documents from that website\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import download_loader\n",
    "\n",
    "WikipediaReader = download_loader(\"WikipediaReader\") # Download the wikipedia reader to fetch documents from that website\n",
    "loader = WikipediaReader() # Create an object of Wikipedia reader\n",
    "documents = loader.load_data(pages=['Natural Language Processing', 'Artificial Intelligence']) # Get documents about NLP and IA\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is ingested as documents, it passes through a processing structure that transforms these documents into `Node` objects. Nodes are data units created from the original documents which constains also metadata and contextual information. In LlamaIndex, there's the `NodeParser` class, designed to convert the content of documents into structured nodes automatically. The `SimpleNodeParser` converts a list of documents objects into nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleja\\AppData\\Local\\Temp\\ipykernel_5740\\1643635700.py:5: DeprecationWarning: Call to deprecated function (or staticmethod) download_loader. (`download_loader()` is deprecated. Please install tool using pip install directly instead.)\n",
      "  WikipediaReader = download_loader(\"WikipediaReader\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core import download_loader\n",
    "\n",
    "# Download the document loader\n",
    "WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "# Create an object to get documents from Wikipedia\n",
    "loader = WikipediaReader()\n",
    "# Load documents\n",
    "loader.load_data(pages=['Natural Language Processing', 'Artificial Intelligence'])\n",
    "\n",
    "# Initialize the parser\n",
    "parser = SimpleNodeParser.from_defaults(chunk_size=512, chunk_overlap=20) # Define number of token per chunk, and overlap between chunks\n",
    "# Parse the documents into nodes\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "print(len(nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that have been generated 58 chunks from the 2 documents fetched from Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing is an initial step for storing information in a database, transforming the unstructured data into embeddings that capture semantic meaning and optimize the data format, so it can be easily accessed and queried. The most popular indexes are these : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracts a summary from each document and stores it with all the nodes in that document. Since itâ€™s not always easy to match small node embeddings with a query, sometimes having a document summary helps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Store Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector store index generates embeddings during index construction to identify the top-k most similar nodes in response to a query.\n",
    "\n",
    "It's suitable for small-scale applications and easily scalable to accommodate larger datasets using high-performance vector databases. \n",
    "\n",
    "We can create a dataset in **Activeloop** and append documents to it by employing the **DeepLakeVectorStore** class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://alejandrotormun/LlamaIndex_intro already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from llama_index.vector_stores.deeplake import DeepLakeVectorStore\n",
    "import os\n",
    "\n",
    "# Change this code to load your ActiveLoop key\n",
    "with open('../data/keys.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    ActiveLoopKey = data['ActiveLoopKey'] # Activeloop personal key\n",
    "    name_org = data['NameOrg'] # Activeloop name org\n",
    "\n",
    "os.environ['ACTIVELOOP_TOKEN'] = ActiveLoopKey\n",
    "\n",
    "# Create/connect an empty dataset in ActiveLoop cloud\n",
    "my_activeloop_dataset_name = \"LlamaIndex_intro\"\n",
    "dataset_path = f\"hub://{name_org}/{my_activeloop_dataset_name}\"\n",
    "vector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous code, we accessed to our personal information : key and organization name's, and then we created an empty dataframe in the ActiveLoop cloud's.\n",
    "\n",
    "Now, we have to create a `StorageContext` object for storing nodes, indexes and embedding vectors. Once we have created it, we have to create a `VectorStoreIndex` class to create the index(generate embeddings) and store the results on the defined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleja\\AppData\\Local\\Temp\\ipykernel_5740\\2142353393.py:11: DeprecationWarning: Call to deprecated function (or staticmethod) download_loader. (`download_loader()` is deprecated. Please install tool using pip install directly instead.)\n",
      "  WikipediaReader = download_loader(\"WikipediaReader\") # Download the wikipedia reader to fetch documents from that website\n",
      "\n",
      "\n",
      "Parsing nodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 86.85it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:20<00:00,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading data to deeplake dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:02<00:00, 13.52it/s]\n",
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://alejandrotormun/LlamaIndex_intro', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape      dtype  compression\n",
      "  -------    -------    -------    -------  ------- \n",
      " embedding  embedding  (31, 4096)  float32   None   \n",
      "    id        text      (31, 1)      str     None   \n",
      " metadata     json      (31, 1)      str     None   \n",
      "   text       text      (31, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    }
   ],
   "source": [
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "from llama_index.core import download_loader\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# We create the storage context from the ActiveLoop's dataset we created in the previous code.\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# We download the documents of interest\n",
    "WikipediaReader = download_loader(\"WikipediaReader\") # Download the wikipedia reader to fetch documents from that website\n",
    "loader = WikipediaReader() # Create an object of Wikipedia reader\n",
    "documents = loader.load_data(pages=['Natural Language Processing', 'Artificial Intelligence']) # Get documents about NLP and IA\n",
    "\n",
    "# Finally, we create the Vector Store index to store the embeddings from the chunks generated from the documents\n",
    "embedding_model = OllamaEmbeddings(model=\"llama3.1:8b\") # Load the model to get the embeddings from the chunks\n",
    "Settings.embed_model = embedding_model # Load it into the setting of llama index\n",
    "index = VectorStoreIndex.from_documents( # Load the embeddings into the database in the cloud\n",
    "    documents=documents,\n",
    "    storage_context=storage_context,\n",
    "    show_progress=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous code, we have used the embedding model of Ollama **llama_3.1:8b**, the lightest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to query the information stored. To do that, we use a query engine, that combines a retriever and a response synthesizer into a pipeline.\n",
    "\n",
    "The pipeline used a string to fetch nodes, and then, send them ot the LLM to generate a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleja\\AppData\\Local\\Temp\\ipykernel_5740\\3660335756.py:8: DeprecationWarning: Call to deprecated function (or staticmethod) download_loader. (`download_loader()` is deprecated. Please install tool using pip install directly instead.)\n",
      "  WikipediaReader = download_loader(\"WikipediaReader\") # Download the wikipedia reader to fetch documents from that website\n",
      "c:\\Users\\aleja\\OneDrive\\Escritorio\\Cursos\\RAG Course ActiveLoop\\RAGCourse\\Lib\\site-packages\\llama_index\\llms\\langchain\\base.py:106: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  output_str = self._llm.predict(prompt, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context information, it appears that \"NLP\" likely stands for \"Natural Language Processing\". This is inferred from the various topics discussed in the text, including approaches to NLP (symbolic, statistical, and neural networks), common NLP tasks (such as syntactic analysis, lexical semantics, and relational semantics), and specific subfields within NLP.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import GPTVectorStoreIndex\n",
    "from llama_index.core import download_loader\n",
    "from langchain_ollama import OllamaLLM\n",
    "from llama_index.core import Settings\n",
    "\n",
    "\n",
    "# We download the documents of interest\n",
    "WikipediaReader = download_loader(\"WikipediaReader\") # Download the wikipedia reader to fetch documents from that website\n",
    "loader = WikipediaReader() # Create an object of Wikipedia reader\n",
    "documents = loader.load_data(pages=['Natural Language Processing', 'Artificial Intelligence']) # Get documents about NLP and IA\n",
    "\n",
    "# Load a local Ollama model to use as LLM \n",
    "llm_model = OllamaLLM(model=\"llama3.1:8b\")\n",
    "Settings.llm = llm_model\n",
    "# Create an index using the loaded documents from Wikipedia\n",
    "index = GPTVectorStoreIndex.from_documents(documents=documents)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What does the NLP stands for?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Routers are used to select the optimal query engine for each task, improving performance and accuracy. These functions are beneficial when dealing with multiple data sources, each holding unique information. Consider an application that employs a SQL database and a Vector Store a it's knowledge base. In this setup, the router can determine which data source is most applicable to the given query. **We will do an example of it in the future**. [link](https://docs.llamaindex.ai/en/stable/module_guides/querying/router/#routers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Indexes Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleja\\AppData\\Local\\Temp\\ipykernel_5740\\494586084.py:6: DeprecationWarning: Call to deprecated function (or staticmethod) download_loader. (`download_loader()` is deprecated. Please install tool using pip install directly instead.)\n",
      "  WikipediaReader = download_loader(\"WikipediaReader\")\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage\n",
    "from llama_index.core import download_loader\n",
    "\n",
    "if not os.path.exists(\"./storage\"):\n",
    "    WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "    loader = WikipediaReader()\n",
    "    documetns = loader.load_data(pages=['Natural Language Processing', 'Artificial Intelligence'])\n",
    "    index = VectorStoreIndex.from_documents(documents=documents)\n",
    "    index.storage_context.persist()\n",
    "else:\n",
    "    # If the index already exists, we'll just load it:\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    "    index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and Main Challenges with naive RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we have learnt the main points of RAG : how to collect data; process it and store it into a vector store database; and finally, how to fetch it to be used by a LLM.\n",
    "\n",
    "\n",
    "The main challenges with naive RAG are these : \n",
    "-   **Bad Retrieval**:\n",
    "    -   *Low Precission* : Not all chunks in retrieved set are relevant. Can cause hallucination.\n",
    "    -   *Low Recall* : Not all releveant chunks are actually retrieved. Lacks enough context for LLM.\n",
    "    -   *Outdated information* : The data is redundant/out of date.\n",
    "-   **Bad Response Generation**:\n",
    "    -   *Hallucination* : Model makes an answer out of context.\n",
    "    -   *Irrelevance* : Model makes an answer that doesn't answer the question.\n",
    "    -   *Toxicity/Bias* : Model makes up an offensive answer.\n",
    "\n",
    "There's also some challenges with RAG, like storing additional data(meta-data), optimize the embeddings or use the LLM for more than just text generation.\n",
    "\n",
    "To solve this challenges, we will see how to increase the tools used in RAG in the next notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGCourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
