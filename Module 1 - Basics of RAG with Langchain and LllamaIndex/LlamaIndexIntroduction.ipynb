{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LlamaIndex** is a framework to connect data and LLM's. This data will be loaded into a some type of structure that later will receive the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of RAG and it's components with LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective of retrieval augmentation is to put some context to the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way RAG works is \n",
    "\n",
    "1.  A documents is loaded and divided into chunks. This chunks processed by a embedding model .Finally,  their vector representations are stored into a vector database. **This first step is the data ingestion**.\n",
    "2.  **The second step is data querying(retrieval+synthesis)**. At this step, chunks of data are extracted from the vector database, based on the similarity with the user's prompt, and given as context to the LLM. You can extract the l-most similar chunks from the vector database and plug them to the synthesis module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the main component's in this framework are these : \n",
    "\n",
    "-   *LlamaHub (Data ingestion)* : Connect to your existing data, like PDF's, doc's, DDBB's...\n",
    "-   *Data Structures* : Store and index your data for different use cases. It can be integrated with different DDBB's, like vector db.\n",
    "-   *Queries* : Retrieve and query over the stored data in the data structures. This includes agents, QA, summarization, ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector store databases enable to store high-dimensional data and provide the essential tools for semantically retrieving relevant documents. These systems analyze the emebddings vectors that encapsulate the entire document's meaning.\n",
    "\n",
    "A primary function is the similarity search. Semantic search transcends traditional keyword matching. It captures the meaning in vectorized representations, and this technique can be applied to all data formats. Once we have the embedded format, we can calculate indexed similarities or capture the context embedded in the query. These ensures that the results are relevant and in line with the contextual and conceptual nuances of the user input's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Connectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Managing data in diverse formats can be challenging, like PDF's, doc's, DDBB's, .csv's... To solve this problem we use the data connectors, also called `Readers`. Readers are responsible for parsing and converting the data into a simplified `Document` representation, **consisting in text and basic metadata**.\n",
    "\n",
    "So, in summary, data connectors are designed to to streamline the data ingestion process, automating the process of fetching data fro differents sources and format it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleja\\AppData\\Local\\Temp\\ipykernel_31244\\2867412336.py:3: DeprecationWarning: Call to deprecated function (or staticmethod) download_loader. (`download_loader()` is deprecated. Please install tool using pip install directly instead.)\n",
      "  WikipediaReader = download_loader(\"WikipediaReader\") # Download the wikipedia reader to fetch documents from that website\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import download_loader\n",
    "\n",
    "WikipediaReader = download_loader(\"WikipediaReader\") # Download the wikipedia reader to fetch documents from that website\n",
    "loader = WikipediaReader() # Create an object of Wikipedia reader\n",
    "documents = loader.load_data(pages=['Natural Language Processing', 'Artificial Intelligence']) # Get documents about NLP and IA\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is ingested as documents, it passes through a processing structure that transforms these documents into `Node` objects. Nodes are data units created from the original documents which constains also metadata and contextual information. In LlamaIndex, there's the `NodeParser` class, designed to convert the content of documents into structured nodes automatically. The `SimpleNodeParser` converts a list of documents objects into nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleja\\AppData\\Local\\Temp\\ipykernel_31244\\1643635700.py:5: DeprecationWarning: Call to deprecated function (or staticmethod) download_loader. (`download_loader()` is deprecated. Please install tool using pip install directly instead.)\n",
      "  WikipediaReader = download_loader(\"WikipediaReader\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core import download_loader\n",
    "\n",
    "# Download the document loader\n",
    "WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "# Create an object to get documents from Wikipedia\n",
    "loader = WikipediaReader()\n",
    "# Load documents\n",
    "loader.load_data(pages=['Natural Language Processing', 'Artificial Intelligence'])\n",
    "\n",
    "# Initialize the parser\n",
    "parser = SimpleNodeParser.from_defaults(chunk_size=512, chunk_overlap=20) # Define number of token per chunk, and overlap between chunks\n",
    "# Parse the documents into nodes\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "print(len(nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that have been generated 48 chunks from the 2 documents fetched from Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGCourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
